[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi üëã I am Youfeng,\na data analysis and data science learner.\nI‚Äôm here to share my study,\nand hope to connect with people from the data engineering and machine learning community.\nPlease reach me on Twitter.\nHappy reading and see you around! üòÉ"
  },
  {
    "objectID": "projects/nbs/generate_names.html",
    "href": "projects/nbs/generate_names.html",
    "title": "Generate English name",
    "section": "",
    "text": "This project train a language model to generate next character based on the previous characters to output an english name."
  },
  {
    "objectID": "projects/nbs/generate_names.html#tools-used",
    "href": "projects/nbs/generate_names.html#tools-used",
    "title": "Generate English name",
    "section": "Tools used",
    "text": "Tools used\nPython, PyTorch."
  },
  {
    "objectID": "projects/nbs/generate_names.html#read-more",
    "href": "projects/nbs/generate_names.html#read-more",
    "title": "Generate English name",
    "section": "Read more",
    "text": "Read more\n Project website ‚ÄÇ  GitHub repo"
  },
  {
    "objectID": "projects/nbs/house_price_prediction.html",
    "href": "projects/nbs/house_price_prediction.html",
    "title": "House Price Prediction",
    "section": "",
    "text": "from fastai.tabular.all import *"
  },
  {
    "objectID": "projects/nbs/eigenface.html",
    "href": "projects/nbs/eigenface.html",
    "title": "EigenFace",
    "section": "",
    "text": "%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image"
  },
  {
    "objectID": "projects/nbs/eigenface.html#load-data",
    "href": "projects/nbs/eigenface.html#load-data",
    "title": "EigenFace",
    "section": "Load data",
    "text": "Load data\n\nX_train = np.loadtxt('../../../data/faces_train.txt')\ny_train = np.loadtxt('../../../data/faces_train_labels.txt')\nX_train.shape, y_train.shape\n\n((280, 1024), (280,))\n\n\n\nX_test = np.loadtxt('../../../data/faces_test.txt')\ny_test= np.loadtxt('../../../data/faces_test_labels.txt')\nX_test.shape, y_test.shape\n\n((120, 1024), (120,))\n\n\n\nUnderstand the data\n\nsample = X_train[0]\nsample.shape\n\n(1024,)\n\n\n\nX_train.shape[0] / 40\n\n7.0\n\n\n\nX_test.shape[0] / 40\n\n3.0"
  },
  {
    "objectID": "projects/nbs/eigenface.html#define-a-function-to-convert-the-data-to-image-array-and-display-the-face-images.",
    "href": "projects/nbs/eigenface.html#define-a-function-to-convert-the-data-to-image-array-and-display-the-face-images.",
    "title": "EigenFace",
    "section": "Define a function to convert the data to image array and display the face images.",
    "text": "Define a function to convert the data to image array and display the face images.\n\ndef show_images(arr, num_person, num_faces):\n    data = arr[:num_person*10]\n    fig, axs = plt.subplots(num_person, num_faces, figsize=(num_faces, num_person))\n    flat_axs = axs.flatten()\n    for i in range(num_person):\n        for j in range(num_faces):\n            flat_axs[i*num_faces+j].imshow(arr[i * int(arr.shape[0] / 40) + j].reshape(32, 32).T)\n            flat_axs[i*num_faces+j].axis('off')\n    plt.set_cmap('gray')\n    plt.tight_layout()\n    plt.show()\n\n\nshow_images(X_train, 3, 5)"
  },
  {
    "objectID": "projects/nbs/eigenface.html#show-mean-face",
    "href": "projects/nbs/eigenface.html#show-mean-face",
    "title": "EigenFace",
    "section": "Show mean face",
    "text": "Show mean face\n\ndef show_image(data):\n    fig, ax = plt.subplots(figsize=(1,1))\n    ax.imshow(data.reshape(32, 32).T)\n    ax.axis('off')\n    plt.set_cmap('gray')\n    plt.tight_layout()\n    plt.show()\n\n\nmean = X_train.mean(0) # shape (1024,)\nshow_image(mean)"
  },
  {
    "objectID": "projects/nbs/eigenface.html#perform-pca-from-covariance-matrix",
    "href": "projects/nbs/eigenface.html#perform-pca-from-covariance-matrix",
    "title": "EigenFace",
    "section": "Perform PCA from covariance matrix",
    "text": "Perform PCA from covariance matrix\n\nCompute eigenvalues and eigenvectors of covariance matrix\n\ndef pca(data):\n    mean = data.mean(0) # shape (1024,)\n    Z = data - mean\n    S = Z.T @ Z\n    eigenvals, eigenvecs = np.linalg.eigh(S) # the eigen values are sorted from smallest to largest\n    reversed_idx = np.argsort(-eigenvals) # get the reversed indices from the largest to smallest\n    eigenvals = eigenvals[reversed_idx]\n    eigenvecs = eigenvecs[:, reversed_idx]\n    return eigenvals, eigenvecs\n\n\n%%time\neigen_vals, eigen_vecs = pca(X_train)\n\nCPU times: user 1.15 s, sys: 63.9 ms, total: 1.21 s\nWall time: 174 ms\n\n\n\neigen_vals.shape, eigen_vecs.shape\n\n((1024,), (1024, 1024))\n\n\n\neigen_vals\n\narray([ 7.59100215e+07,  4.19397005e+07,  2.54955933e+07, ...,\n       -5.02190332e-09, -7.09703055e-09, -7.63365465e-09])\n\n\n\neigen_vecs[:5]\n\narray([[-0.01287756, -0.05501646,  0.0041873 , ...,  0.        ,\n         0.        ,  0.        ],\n       [-0.01462181, -0.06097046, -0.00761386, ...,  0.04503731,\n         0.06237077,  0.21112058],\n       [-0.01704114, -0.06917858, -0.01997429, ...,  0.08273158,\n         0.36417853,  0.14920292],\n       [-0.02048617, -0.07692255, -0.02277752, ..., -0.04538269,\n        -0.0119913 , -0.37879662],\n       [-0.02213695, -0.07991882, -0.02080318, ...,  0.02432163,\n         0.13519464,  0.02273907]])\n\n\n\n\nShow top 5 eigenfaces\n\ndef show_pc_images(data):\n    nrows, ncols = data.shape\n    fig, axs = plt.subplots(1, ncols, figsize=(ncols, 1.5))\n    flat_axs = axs.flatten()\n    for i in range(ncols):\n        flat_axs[i].imshow(data[:, i].reshape(32, 32).T)\n        flat_axs[i].axis('off')\n        flat_axs[i].set_title(f'pc{i}')\n    plt.set_cmap('gray')\n    plt.tight_layout()\n    plt.show()\n\n\nshow_pc_images(eigen_vecs[:, :5])"
  },
  {
    "objectID": "projects/nbs/eigenface.html#perform-pca-with-svd",
    "href": "projects/nbs/eigenface.html#perform-pca-with-svd",
    "title": "EigenFace",
    "section": "Perform PCA with SVD",
    "text": "Perform PCA with SVD\n\ndef svd(data):\n    data_centered = data - data.mean(0)\n    U, s, Vt = np.linalg.svd(data_centered)\n    return U, s, Vt.T\n\n\n%%time\nU, s, V = svd(X_train)\n\nCPU times: user 837 ms, sys: 26.8 ms, total: 864 ms\nWall time: 124 ms\n\n\n\nU.shape, s.shape, V.shape\n\n((280, 280), (280,), (1024, 1024))\n\n\n\n\n\n\n\n\nNote\n\n\n\nV is the same as the eigen vector matrix, and \\(s^2\\) is equal to the corresponding eigen values.\n\n\n\nCheck if the top 5 eigenvalues are equal\n\nnp.square(s[:5]), eigen_vals[:5]\n\n(array([75910021.54729882, 41939700.47489863, 25495593.34635307,\n        17539063.72985784, 12170662.99105223]),\n array([75910021.54729888, 41939700.47489879, 25495593.34635304,\n        17539063.72985789, 12170662.99105226]))\n\n\n\nnp.allclose(np.square(s[:5]), eigen_vals[:5])\n\nTrue\n\n\n\n\nCheck if the top 5 eigenvectors are equal\n\nV[:, :5], eigen_vecs[:, :5]\n\n(array([[-0.01287756,  0.05501646,  0.0041873 ,  0.00651911,  0.0704008 ],\n        [-0.01462181,  0.06097046, -0.00761386,  0.00091407,  0.07140231],\n        [-0.01704114,  0.06917858, -0.01997429, -0.00113944,  0.0715405 ],\n        ...,\n        [ 0.00308199, -0.04617929, -0.03671734,  0.03130467,  0.07777847],\n        [ 0.00747202, -0.0494147 , -0.04110058,  0.03836678,  0.07902728],\n        [ 0.0109414 , -0.05125083, -0.03781413,  0.04223883,  0.07396954]]),\n array([[-0.01287756, -0.05501646,  0.0041873 , -0.00651911,  0.0704008 ],\n        [-0.01462181, -0.06097046, -0.00761386, -0.00091407,  0.07140231],\n        [-0.01704114, -0.06917858, -0.01997429,  0.00113944,  0.0715405 ],\n        ...,\n        [ 0.00308199,  0.04617929, -0.03671734, -0.03130467,  0.07777847],\n        [ 0.00747202,  0.0494147 , -0.04110058, -0.03836678,  0.07902728],\n        [ 0.0109414 ,  0.05125083, -0.03781413, -0.04223883,  0.07396954]]))\n\n\n\n\n\n\n\n\nTip\n\n\n\nSince eigenvectors can be same values, but with opposite signs, we compare the absolute values.\n\n\n\nnp.allclose(np.abs(V[:, :5]), np.abs(eigen_vecs[:, :5]))\n\nTrue\n\n\n\n\nShow top 5 eigenfaces\n\nshow_pc_images(V[:, :5])"
  },
  {
    "objectID": "projects/nbs/eigenface.html#projecting-3-persons-faces-data-down-to-2-dimensions-and-plot-them.",
    "href": "projects/nbs/eigenface.html#projecting-3-persons-faces-data-down-to-2-dimensions-and-plot-them.",
    "title": "EigenFace",
    "section": "Projecting 3 persons‚Äô faces data down to 2 dimensions and plot them.",
    "text": "Projecting 3 persons‚Äô faces data down to 2 dimensions and plot them.\n\nT = X_train[:7*3, :] @ V[:, :2]\nT.shape\n\n(21, 2)\n\n\n\ny = y_train[:7*3].reshape(-1, 1)\ny.shape\n\n(21, 1)\n\n\n\ndata_2d = pd.DataFrame(data=np.concatenate((T, y), axis=1), columns=['pc1', 'pc2', 'person'])\ndata_2d.head()\n\n\n\n\n\n  \n    \n      \n      pc1\n      pc2\n      person\n    \n  \n  \n    \n      0\n      -4907.496730\n      458.054067\n      1.0\n    \n    \n      1\n      -4344.197376\n      1289.734627\n      1.0\n    \n    \n      2\n      -4775.345055\n      577.090674\n      1.0\n    \n    \n      3\n      -4492.115417\n      -930.775823\n      1.0\n    \n    \n      4\n      -4639.398032\n      840.018975\n      1.0\n    \n  \n\n\n\n\n\nfig, ax = plt.subplots(figsize=(8, 5))\nsns.scatterplot(data=data_2d, x='pc1', y='pc2', hue='person', palette=['blue', 'red', 'green'], ax=ax)\nplt.show()\n\n\n\n\nThe above plot shows that the data is separable."
  },
  {
    "objectID": "projects/nbs/eigenface.html#explained-variance-ratio",
    "href": "projects/nbs/eigenface.html#explained-variance-ratio",
    "title": "EigenFace",
    "section": "Explained variance ratio",
    "text": "Explained variance ratio\n\ns.shape\n\n(280,)\n\n\n\ns_norm = s / s.sum()\nplt.plot(np.cumsum(s_norm))\nplt.xlabel('singular values')\nplt.ylabel('cumulative sum')\nplt.xlim(0, 280)\nplt.ylim(0, 1)\nplt.show()"
  },
  {
    "objectID": "projects/nbs/eigenface.html#reconstruct-face-from-top-principle-components",
    "href": "projects/nbs/eigenface.html#reconstruct-face-from-top-principle-components",
    "title": "EigenFace",
    "section": "Reconstruct face from top principle components",
    "text": "Reconstruct face from top principle components\n\nOriginal face\n\npicked_face = X_train[0]\nshow_image(picked_face)\n\n\n\n\n\n\nReconstruct_face method 1\n\nk = 20 # the kth eigenvector\n\n\nreconstruct_face1 = (picked_face - mean) @ eigen_vecs[:, :k] @ eigen_vecs[:, :k].T + mean\nshow_image(reconstruct_face1)\n\n\n\n\n\n\nReconstruct_face method 2\n\nS = np.diag(s)\nS.shape\n\n(280, 280)\n\n\n\nreconstruct_face2 = U[0, :k] @ S[:k, :k] @ (V.T)[:k, :] + mean\nreconstruct_face2.shape\n\n(1024,)\n\n\n\nshow_image(reconstruct_face2)\n\n\n\n\n\n\nConfirm the two reconstruct_faces are the same\n\nnp.allclose(reconstruct_face1, reconstruct_face2)\n\nTrue"
  },
  {
    "objectID": "projects/nbs/extract_dataframe_from_txt_file.html",
    "href": "projects/nbs/extract_dataframe_from_txt_file.html",
    "title": "Extracting and grouping text info by category",
    "section": "",
    "text": "The text file we will work on is a data description file which is part of the Kaggle dataset. The file is used to describe all of the 79 explanatory variables (features) and the meaning of each corresponding category variable‚Äôs code."
  },
  {
    "objectID": "projects/nbs/extract_dataframe_from_txt_file.html#the-goal",
    "href": "projects/nbs/extract_dataframe_from_txt_file.html#the-goal",
    "title": "Extracting and grouping text info by category",
    "section": "The goal",
    "text": "The goal\nThe goal of this project is to extract the information about the explanatory variables and category variables‚Äô codes, then create a seperate pandas dataframe for each one and save as csv file for downstream tasks."
  },
  {
    "objectID": "projects/nbs/extract_dataframe_from_txt_file.html#import-packages",
    "href": "projects/nbs/extract_dataframe_from_txt_file.html#import-packages",
    "title": "Extracting and grouping text info by category",
    "section": "Import packages",
    "text": "Import packages\n\nimport pandas as pd\nimport numpy as np\nimport re\nfrom fastcore.utils import *"
  },
  {
    "objectID": "projects/nbs/extract_dataframe_from_txt_file.html#load-the-data",
    "href": "projects/nbs/extract_dataframe_from_txt_file.html#load-the-data",
    "title": "Extracting and grouping text info by category",
    "section": "Load the data",
    "text": "Load the data\nThe path of the Kaggle dataset.\n\npath = Path('./house-prices-advanced-regression-techniques/')\n\nHave a look at the data\n\ntrain_df = pd.read_csv(path/'train.csv')\ntrain_df.shape\n\n(1460, 81)\n\n\n\ntrain_df.tail().iloc[:, -5:]\n\n\n\n\n\n  \n    \n      \n      MoSold\n      YrSold\n      SaleType\n      SaleCondition\n      SalePrice\n    \n  \n  \n    \n      1455\n      8\n      2007\n      WD\n      Normal\n      175000\n    \n    \n      1456\n      2\n      2010\n      WD\n      Normal\n      210000\n    \n    \n      1457\n      5\n      2010\n      WD\n      Normal\n      266500\n    \n    \n      1458\n      4\n      2010\n      WD\n      Normal\n      142125\n    \n    \n      1459\n      6\n      2008\n      WD\n      Normal\n      147500\n    \n  \n\n\n\n\n\ntest_df = pd.read_csv(path/'test.csv')\ntest_df.shape\n\n(1459, 80)\n\n\n\ntest_df.tail().iloc[:, -5:]\n\n\n\n\n\n  \n    \n      \n      MiscVal\n      MoSold\n      YrSold\n      SaleType\n      SaleCondition\n    \n  \n  \n    \n      1454\n      0\n      6\n      2006\n      WD\n      Normal\n    \n    \n      1455\n      0\n      4\n      2006\n      WD\n      Abnorml\n    \n    \n      1456\n      0\n      9\n      2006\n      WD\n      Abnorml\n    \n    \n      1457\n      700\n      7\n      2006\n      WD\n      Normal\n    \n    \n      1458\n      0\n      11\n      2006\n      WD\n      Normal\n    \n  \n\n\n\n\n\ntrain_df.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1460 entries, 0 to 1459\nData columns (total 81 columns):\n #   Column         Non-Null Count  Dtype  \n---  ------         --------------  -----  \n 0   Id             1460 non-null   int64  \n 1   MSSubClass     1460 non-null   int64  \n 2   MSZoning       1460 non-null   object \n 3   LotFrontage    1201 non-null   float64\n 4   LotArea        1460 non-null   int64  \n 5   Street         1460 non-null   object \n 6   Alley          91 non-null     object \n 7   LotShape       1460 non-null   object \n 8   LandContour    1460 non-null   object \n 9   Utilities      1460 non-null   object \n 10  LotConfig      1460 non-null   object \n 11  LandSlope      1460 non-null   object \n 12  Neighborhood   1460 non-null   object \n 13  Condition1     1460 non-null   object \n 14  Condition2     1460 non-null   object \n 15  BldgType       1460 non-null   object \n 16  HouseStyle     1460 non-null   object \n 17  OverallQual    1460 non-null   int64  \n 18  OverallCond    1460 non-null   int64  \n 19  YearBuilt      1460 non-null   int64  \n 20  YearRemodAdd   1460 non-null   int64  \n 21  RoofStyle      1460 non-null   object \n 22  RoofMatl       1460 non-null   object \n 23  Exterior1st    1460 non-null   object \n 24  Exterior2nd    1460 non-null   object \n 25  MasVnrType     1452 non-null   object \n 26  MasVnrArea     1452 non-null   float64\n 27  ExterQual      1460 non-null   object \n 28  ExterCond      1460 non-null   object \n 29  Foundation     1460 non-null   object \n 30  BsmtQual       1423 non-null   object \n 31  BsmtCond       1423 non-null   object \n 32  BsmtExposure   1422 non-null   object \n 33  BsmtFinType1   1423 non-null   object \n 34  BsmtFinSF1     1460 non-null   int64  \n 35  BsmtFinType2   1422 non-null   object \n 36  BsmtFinSF2     1460 non-null   int64  \n 37  BsmtUnfSF      1460 non-null   int64  \n 38  TotalBsmtSF    1460 non-null   int64  \n 39  Heating        1460 non-null   object \n 40  HeatingQC      1460 non-null   object \n 41  CentralAir     1460 non-null   object \n 42  Electrical     1459 non-null   object \n 43  1stFlrSF       1460 non-null   int64  \n 44  2ndFlrSF       1460 non-null   int64  \n 45  LowQualFinSF   1460 non-null   int64  \n 46  GrLivArea      1460 non-null   int64  \n 47  BsmtFullBath   1460 non-null   int64  \n 48  BsmtHalfBath   1460 non-null   int64  \n 49  FullBath       1460 non-null   int64  \n 50  HalfBath       1460 non-null   int64  \n 51  BedroomAbvGr   1460 non-null   int64  \n 52  KitchenAbvGr   1460 non-null   int64  \n 53  KitchenQual    1460 non-null   object \n 54  TotRmsAbvGrd   1460 non-null   int64  \n 55  Functional     1460 non-null   object \n 56  Fireplaces     1460 non-null   int64  \n 57  FireplaceQu    770 non-null    object \n 58  GarageType     1379 non-null   object \n 59  GarageYrBlt    1379 non-null   float64\n 60  GarageFinish   1379 non-null   object \n 61  GarageCars     1460 non-null   int64  \n 62  GarageArea     1460 non-null   int64  \n 63  GarageQual     1379 non-null   object \n 64  GarageCond     1379 non-null   object \n 65  PavedDrive     1460 non-null   object \n 66  WoodDeckSF     1460 non-null   int64  \n 67  OpenPorchSF    1460 non-null   int64  \n 68  EnclosedPorch  1460 non-null   int64  \n 69  3SsnPorch      1460 non-null   int64  \n 70  ScreenPorch    1460 non-null   int64  \n 71  PoolArea       1460 non-null   int64  \n 72  PoolQC         7 non-null      object \n 73  Fence          281 non-null    object \n 74  MiscFeature    54 non-null     object \n 75  MiscVal        1460 non-null   int64  \n 76  MoSold         1460 non-null   int64  \n 77  YrSold         1460 non-null   int64  \n 78  SaleType       1460 non-null   object \n 79  SaleCondition  1460 non-null   object \n 80  SalePrice      1460 non-null   int64  \ndtypes: float64(3), int64(35), object(43)\nmemory usage: 924.0+ KB\n\n\nFrom the above, we know that train set (except the dependent valiable SalePrice) and test set both have 79 explanatory variables (excluding Id).\nAll the independent variables are:\n\nind_vars = train_df.columns[1:-1]\nind_vars\n\nIndex(['MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'Street', 'Alley',\n       'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope',\n       'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle',\n       'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'RoofStyle',\n       'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'MasVnrArea',\n       'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond',\n       'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1', 'BsmtFinType2',\n       'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'Heating', 'HeatingQC',\n       'CentralAir', 'Electrical', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF',\n       'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath',\n       'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual', 'TotRmsAbvGrd',\n       'Functional', 'Fireplaces', 'FireplaceQu', 'GarageType', 'GarageYrBlt',\n       'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual', 'GarageCond',\n       'PavedDrive', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch',\n       'ScreenPorch', 'PoolArea', 'PoolQC', 'Fence', 'MiscFeature', 'MiscVal',\n       'MoSold', 'YrSold', 'SaleType', 'SaleCondition'],\n      dtype='object')\n\n\n\nlen(ind_vars)\n\n79\n\n\nAmong the independent variables, some are numerical variables, others are category variables.\n\nnum_cols = train_df.select_dtypes(include=np.number).columns # numeric columns\nnum_cols\n\nIndex(['Id', 'MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual',\n       'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1',\n       'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF',\n       'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath',\n       'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd',\n       'Fireplaces', 'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF',\n       'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea',\n       'MiscVal', 'MoSold', 'YrSold', 'SalePrice'],\n      dtype='object')\n\n\n\ncat_cols = train_df.select_dtypes(include='object').columns # category columns\ncat_cols\n\nIndex(['MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities',\n       'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2',\n       'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st',\n       'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation',\n       'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n       'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual',\n       'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual',\n       'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature',\n       'SaleType', 'SaleCondition'],\n      dtype='object')\n\n\nLet have a look at the data description file: data_description.txt\nThe first 30 lines is as below:\n\n!head -n 30 {path/'data_description.txt'}\n\nMSSubClass: Identifies the type of dwelling involved in the sale.   \n\n        20  1-STORY 1946 & NEWER ALL STYLES\n        30  1-STORY 1945 & OLDER\n        40  1-STORY W/FINISHED ATTIC ALL AGES\n        45  1-1/2 STORY - UNFINISHED ALL AGES\n        50  1-1/2 STORY FINISHED ALL AGES\n        60  2-STORY 1946 & NEWER\n        70  2-STORY 1945 & OLDER\n        75  2-1/2 STORY ALL AGES\n        80  SPLIT OR MULTI-LEVEL\n        85  SPLIT FOYER\n        90  DUPLEX - ALL STYLES AND AGES\n       120  1-STORY PUD (Planned Unit Development) - 1946 & NEWER\n       150  1-1/2 STORY PUD - ALL AGES\n       160  2-STORY PUD - 1946 & NEWER\n       180  PUD - MULTILEVEL - INCL SPLIT LEV/FOYER\n       190  2 FAMILY CONVERSION - ALL STYLES AND AGES\n\nMSZoning: Identifies the general zoning classification of the sale.\n        \n       A    Agriculture\n       C    Commercial\n       FV   Floating Village Residential\n       I    Industrial\n       RH   Residential High Density\n       RL   Residential Low Density\n       RP   Residential Low Density Park \n       RM   Residential Medium Density\n    \n\n\nLet grep the lines with MS\n\n!grep MS {path/'data_description.txt'}\n\nMSSubClass: Identifies the type of dwelling involved in the sale.   \nMSZoning: Identifies the general zoning classification of the sale.\n\n\n\n!rg MS {path/'data_description.txt'}\n\n1:MSSubClass: Identifies the type of dwelling involved in the sale. \n20:MSZoning: Identifies the general zoning classification of the sale.\n\n\nRead the text file and split lines into a list\n\ndesc = (path/'data_description.txt').read_text().splitlines()\ndesc[:30]\n\n['MSSubClass: Identifies the type of dwelling involved in the sale.\\t',\n '',\n '        20\\t1-STORY 1946 & NEWER ALL STYLES',\n '        30\\t1-STORY 1945 & OLDER',\n '        40\\t1-STORY W/FINISHED ATTIC ALL AGES',\n '        45\\t1-1/2 STORY - UNFINISHED ALL AGES',\n '        50\\t1-1/2 STORY FINISHED ALL AGES',\n '        60\\t2-STORY 1946 & NEWER',\n '        70\\t2-STORY 1945 & OLDER',\n '        75\\t2-1/2 STORY ALL AGES',\n '        80\\tSPLIT OR MULTI-LEVEL',\n '        85\\tSPLIT FOYER',\n '        90\\tDUPLEX - ALL STYLES AND AGES',\n '       120\\t1-STORY PUD (Planned Unit Development) - 1946 & NEWER',\n '       150\\t1-1/2 STORY PUD - ALL AGES',\n '       160\\t2-STORY PUD - 1946 & NEWER',\n '       180\\tPUD - MULTILEVEL - INCL SPLIT LEV/FOYER',\n '       190\\t2 FAMILY CONVERSION - ALL STYLES AND AGES',\n '',\n 'MSZoning: Identifies the general zoning classification of the sale.',\n '\\t\\t',\n '       A\\tAgriculture',\n '       C\\tCommercial',\n '       FV\\tFloating Village Residential',\n '       I\\tIndustrial',\n '       RH\\tResidential High Density',\n '       RL\\tResidential Low Density',\n '       RP\\tResidential Low Density Park ',\n '       RM\\tResidential Medium Density',\n '\\t']\n\n\nRemove all the empty lines.\n\ndesc = [i for i in desc if len(i.strip()) != 0]\ndesc[:30]\n\n['MSSubClass: Identifies the type of dwelling involved in the sale.\\t',\n '        20\\t1-STORY 1946 & NEWER ALL STYLES',\n '        30\\t1-STORY 1945 & OLDER',\n '        40\\t1-STORY W/FINISHED ATTIC ALL AGES',\n '        45\\t1-1/2 STORY - UNFINISHED ALL AGES',\n '        50\\t1-1/2 STORY FINISHED ALL AGES',\n '        60\\t2-STORY 1946 & NEWER',\n '        70\\t2-STORY 1945 & OLDER',\n '        75\\t2-1/2 STORY ALL AGES',\n '        80\\tSPLIT OR MULTI-LEVEL',\n '        85\\tSPLIT FOYER',\n '        90\\tDUPLEX - ALL STYLES AND AGES',\n '       120\\t1-STORY PUD (Planned Unit Development) - 1946 & NEWER',\n '       150\\t1-1/2 STORY PUD - ALL AGES',\n '       160\\t2-STORY PUD - 1946 & NEWER',\n '       180\\tPUD - MULTILEVEL - INCL SPLIT LEV/FOYER',\n '       190\\t2 FAMILY CONVERSION - ALL STYLES AND AGES',\n 'MSZoning: Identifies the general zoning classification of the sale.',\n '       A\\tAgriculture',\n '       C\\tCommercial',\n '       FV\\tFloating Village Residential',\n '       I\\tIndustrial',\n '       RH\\tResidential High Density',\n '       RL\\tResidential Low Density',\n '       RP\\tResidential Low Density Park ',\n '       RM\\tResidential Medium Density',\n 'LotFrontage: Linear feet of street connected to property',\n 'LotArea: Lot size in square feet',\n 'Street: Type of road access to property',\n '       Grvl\\tGravel\\t']\n\n\nAfter looking at the above text file, we noticed that the data types of some independent variables are incorrect.\nLet fix it."
  },
  {
    "objectID": "projects/nbs/extract_dataframe_from_txt_file.html#define-a-function-to-extract-all-the-descriptions-about-the-79-explanatory-variables-and-check-the-data-type.",
    "href": "projects/nbs/extract_dataframe_from_txt_file.html#define-a-function-to-extract-all-the-descriptions-about-the-79-explanatory-variables-and-check-the-data-type.",
    "title": "Extracting and grouping text info by category",
    "section": "Define a function to extract all the descriptions about the 79 explanatory variables and check the data type.",
    "text": "Define a function to extract all the descriptions about the 79 explanatory variables and check the data type.\n\ndef var_desc(txt, ind_vars):\n    d = {}\n    cat = {}\n    for i, t in enumerate(txt):\n        rt = re.findall(r'(\\S+): ([^\\t?]+)', t)\n        if rt:\n            if rt[0][0] in ind_vars:\n                d[rt[0][0]] = rt[0][1]\n                if txt[i+1].startswith(' '):\n                    cat[rt[0][0]] = 'Category'\n    df = pd.DataFrame(d.items(), columns=['Variable', 'Variabe_Description'])\n    df['Variable_Type'] = df.Variable.map(cat).fillna('Numerical')\n    return df\n\n\ndf = var_desc(desc, ind_vars)\ndf.head()\n\n\n\n\n\n  \n    \n      \n      Variable\n      Variabe_Description\n      Variable_Type\n    \n  \n  \n    \n      0\n      MSSubClass\n      Identifies the type of dwelling involved in th...\n      Category\n    \n    \n      1\n      MSZoning\n      Identifies the general zoning classification o...\n      Category\n    \n    \n      2\n      LotFrontage\n      Linear feet of street connected to property\n      Numerical\n    \n    \n      3\n      LotArea\n      Lot size in square feet\n      Numerical\n    \n    \n      4\n      Street\n      Type of road access to property\n      Category\n    \n  \n\n\n\n\n\nlen(df)\n\n77\n\n\n\ndf.to_csv(path/f'vars_desc.csv', index=False)\n\n\ncat_vars = df['Variable'][df['Variable_Type'] == 'Category'].values\ncat_vars, len(cat_vars)\n\n(array(['MSSubClass', 'MSZoning', 'Street', 'Alley', 'LotShape',\n        'LandContour', 'Utilities', 'LotConfig', 'LandSlope',\n        'Neighborhood', 'Condition1', 'Condition2', 'BldgType',\n        'HouseStyle', 'OverallQual', 'OverallCond', 'RoofStyle',\n        'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType',\n        'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond',\n        'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating',\n        'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual',\n        'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish',\n        'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', 'Fence',\n        'MiscFeature', 'SaleType', 'SaleCondition'], dtype=object),\n 46)\n\n\n\ncat_cols, len(cat_cols)\n\n(Index(['MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities',\n        'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2',\n        'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st',\n        'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation',\n        'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n        'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual',\n        'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual',\n        'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature',\n        'SaleType', 'SaleCondition'],\n       dtype='object'),\n 43)\n\n\n\n[i for i in cat_cols if i not in cat_vars]\n\n[]\n\n\n\n[i for i in cat_vars if i not in cat_cols]\n\n['MSSubClass', 'OverallQual', 'OverallCond']\n\n\nAfter comparing the results of data types from the original train set and text file, we get the category variables.\n\ncat_vars\n\narray(['MSSubClass', 'MSZoning', 'Street', 'Alley', 'LotShape',\n       'LandContour', 'Utilities', 'LotConfig', 'LandSlope',\n       'Neighborhood', 'Condition1', 'Condition2', 'BldgType',\n       'HouseStyle', 'OverallQual', 'OverallCond', 'RoofStyle',\n       'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType',\n       'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond',\n       'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating',\n       'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual',\n       'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish',\n       'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', 'Fence',\n       'MiscFeature', 'SaleType', 'SaleCondition'], dtype=object)"
  },
  {
    "objectID": "projects/nbs/extract_dataframe_from_txt_file.html#define-a-function-to-extract-all-the-information-about-each-category-variables-codes.",
    "href": "projects/nbs/extract_dataframe_from_txt_file.html#define-a-function-to-extract-all-the-information-about-each-category-variables-codes.",
    "title": "Extracting and grouping text info by category",
    "section": "Define a function to extract all the information about each category variable‚Äôs codes.",
    "text": "Define a function to extract all the information about each category variable‚Äôs codes.\n\ndef cat_desc(txt, col_name):\n    d = {}\n    # find the column name match the category variable\n    for i in range(len(txt)):\n        rt1 = re.match(col_name, txt[i])\n        if rt1:\n#             print(rt1.group(0), i) # debugging\n            j = i+1\n    # extract the info about the the corresponding category variable\n    for t in txt[j:]:\n        if t.startswith(' '): # check if the text start with white space\n            # find two parts (the code and code description) using regex\n            # the code might be 'NA' , digits or any word characters\n            rt2 = re.findall(r'\\s+(\\d+\\.\\d+\\w+|NA\\s+|\\w+\\&?\\w*?\\s*?)\\t(.*)', t, flags=re.IGNORECASE) \n            d[rt2[0][0].strip()] = (rt2[0][1]).strip()\n        else:\n            break\n    # return a dataframe of the category variable\n    return pd.DataFrame(d.items(), columns=[col_name, col_name + '_Description'])\n\nTest the function with category variable MSSubClass and SaleCondition\n\ncol_name = 'MSSubClass'\ncat_desc(desc, col_name)\n\n\n\n\n\n  \n    \n      \n      MSSubClass\n      MSSubClass_Description\n    \n  \n  \n    \n      0\n      20\n      1-STORY 1946 & NEWER ALL STYLES\n    \n    \n      1\n      30\n      1-STORY 1945 & OLDER\n    \n    \n      2\n      40\n      1-STORY W/FINISHED ATTIC ALL AGES\n    \n    \n      3\n      45\n      1-1/2 STORY - UNFINISHED ALL AGES\n    \n    \n      4\n      50\n      1-1/2 STORY FINISHED ALL AGES\n    \n    \n      5\n      60\n      2-STORY 1946 & NEWER\n    \n    \n      6\n      70\n      2-STORY 1945 & OLDER\n    \n    \n      7\n      75\n      2-1/2 STORY ALL AGES\n    \n    \n      8\n      80\n      SPLIT OR MULTI-LEVEL\n    \n    \n      9\n      85\n      SPLIT FOYER\n    \n    \n      10\n      90\n      DUPLEX - ALL STYLES AND AGES\n    \n    \n      11\n      120\n      1-STORY PUD (Planned Unit Development) - 1946 ...\n    \n    \n      12\n      150\n      1-1/2 STORY PUD - ALL AGES\n    \n    \n      13\n      160\n      2-STORY PUD - 1946 & NEWER\n    \n    \n      14\n      180\n      PUD - MULTILEVEL - INCL SPLIT LEV/FOYER\n    \n    \n      15\n      190\n      2 FAMILY CONVERSION - ALL STYLES AND AGES\n    \n  \n\n\n\n\n\ncol_name = 'SaleCondition'\ncat_desc(desc, col_name)\n\n\n\n\n\n  \n    \n      \n      SaleCondition\n      SaleCondition_Description\n    \n  \n  \n    \n      0\n      Normal\n      Normal Sale\n    \n    \n      1\n      Abnorml\n      Abnormal Sale -  trade, foreclosure, short sale\n    \n    \n      2\n      AdjLand\n      Adjoining Land Purchase\n    \n    \n      3\n      Alloca\n      Allocation - two linked properties with separa...\n    \n    \n      4\n      Family\n      Sale between family members\n    \n    \n      5\n      Partial\n      Home was not completed when last assessed (ass..."
  },
  {
    "objectID": "projects/nbs/extract_dataframe_from_txt_file.html#create-pandas-dataframe-and-save-them-as-csv-files.",
    "href": "projects/nbs/extract_dataframe_from_txt_file.html#create-pandas-dataframe-and-save-them-as-csv-files.",
    "title": "Extracting and grouping text info by category",
    "section": "Create pandas dataframe and save them as csv files.",
    "text": "Create pandas dataframe and save them as csv files.\n\nfor cat_var in cat_vars:\n    cat_df = cat_desc(desc, cat_var)\n    cat_df.to_csv(path/f'{cat_var}.csv', index=False)\n\n\n!ls {path}\n\nAlley.csv             Foundation.csv        MiscFeature.csv\nBldgType.csv          Functional.csv        Neighborhood.csv\nBsmtCond.csv          GarageCond.csv        OverallCond.csv\nBsmtExposure.csv      GarageFinish.csv      OverallQual.csv\nBsmtFinType1.csv      GarageQual.csv        PavedDrive.csv\nBsmtFinType2.csv      GarageType.csv        PoolQC.csv\nBsmtQual.csv          Heating.csv           RoofMatl.csv\nCentralAir.csv        HeatingQC.csv         RoofStyle.csv\nCondition1.csv        HouseStyle.csv        SaleCondition.csv\nCondition2.csv        KitchenQual.csv       SaleType.csv\nElectrical.csv        LandContour.csv       Street.csv\nExterCond.csv         LandSlope.csv         Utilities.csv\nExterQual.csv         LotConfig.csv         data_description.txt\nExterior1st.csv       LotShape.csv          sample_submission.csv\nExterior2nd.csv       MSSubClass.csv        test.csv\nFence.csv             MSZoning.csv          train.csv\nFireplaceQu.csv       MasVnrType.csv        vars_desc.csv"
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "Extracting and grouping text info by category\n\n\n\ndata cleaning\n\n\nregex\n\n\n\nThis notebook shows matching text pattern and extract text and processing strings using Regex.\n\n\n\nOct 23, 2022\n\n\n\n\n\n\n\n\n\n\n\nGenerate English name\n\n\n\ndeep learning\n\n\nnlp\n\n\n\nTrain a language model to generate next character based on the previous characters to output an english name.\n\n\n\nSep 23, 2022\n\n\n\n\n\n\n\n\n\n\n\nEigenFace\n\n\n\nPCA\n\n\nSVD\n\n\n\nThis notebook will dive into eigenvalues and eigenvectors with covariance matrix. We also compare this method with SVD to understand what happens under the hood.\n\n\n\nMay 26, 2022\n\n\n\n\n\n\n\n\n\n\n\nHouse Price Prediction\n\n\n\nmachine learning\n\n\n\nThis notebook will\n\n\n\nMay 23, 2022\n\n\n\n\n\n\n\n\n\n\n\nPower BI\n\n\n\ndata visualization\n\n\n\nThis notebook will show how to use Power BI\n\n\n\nMay 23, 2022\n\n\n\n\n\n\n\n\n\n\n\nWords Clouds\n\n\n\nexploratory data analysis\n\n\n\nThis notebook will show how to create word clouds in python\n\n\n\nMay 23, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/nbs/connect_db.html",
    "href": "posts/nbs/connect_db.html",
    "title": "Connect to MySql with Pandas",
    "section": "",
    "text": "import sqlalchemy as sqla\nimport pandas as pd"
  },
  {
    "objectID": "posts/nbs/connect_db.html#connect-to-a-database",
    "href": "posts/nbs/connect_db.html#connect-to-a-database",
    "title": "Connect to MySql with Pandas",
    "section": "Connect to a database",
    "text": "Connect to a database\n\ndb = sqla.create_engine('mysql+mysqlconnector://root:@localhost:3306/farmers_market')"
  },
  {
    "objectID": "posts/nbs/connect_db.html#get-all-the-tables-in-the-database",
    "href": "posts/nbs/connect_db.html#get-all-the-tables-in-the-database",
    "title": "Connect to MySql with Pandas",
    "section": "Get all the tables in the database",
    "text": "Get all the tables in the database\n\ndb_tables = pd.read_sql('show tables from farmers_market', db)\ndb_tables\n\n\n\n\n\n  \n    \n      \n      Tables_in_farmers_market\n    \n  \n  \n    \n      0\n      booth\n    \n    \n      1\n      customer\n    \n    \n      2\n      customer_purchases\n    \n    \n      3\n      market_date_info\n    \n    \n      4\n      product\n    \n    \n      5\n      product_category\n    \n    \n      6\n      vendor\n    \n    \n      7\n      vendor_booth_assignments\n    \n    \n      8\n      vendor_inventory\n    \n    \n      9\n      zip_data"
  },
  {
    "objectID": "posts/nbs/connect_db.html#get-information-about-the-schema-of-each-table",
    "href": "posts/nbs/connect_db.html#get-information-about-the-schema-of-each-table",
    "title": "Connect to MySql with Pandas",
    "section": "Get information about the schema of each table",
    "text": "Get information about the schema of each table\n\ntables = db_tables['Tables_in_farmers_market']\n\nfor table in tables:\n    desc = pd.read_sql('describe {}'.format(table), db)\n    print('*'*30, table, '*'*30)\n    print(desc, '\\n')\n\n****************************** booth ******************************\n               Field          Type Null  Key Default Extra\n0       booth_number       int(11)   NO  PRI    None      \n1  booth_price_level   varchar(45)   NO         None      \n2  booth_description  varchar(255)   NO         None      \n3         booth_type   varchar(45)   NO         None       \n\n****************************** customer ******************************\n                 Field         Type Null  Key Default Extra\n0          customer_id      int(11)   NO  PRI    None      \n1  customer_first_name  varchar(45)  YES         None      \n2   customer_last_name  varchar(45)  YES         None      \n3         customer_zip  varchar(45)  YES         None       \n\n****************************** customer_purchases ******************************\n                      Field           Type Null  Key Default Extra\n0                product_id        int(11)   NO  PRI    None      \n1                 vendor_id        int(11)   NO  PRI    None      \n2               market_date           date   NO  PRI    None      \n3               customer_id        int(11)   NO  PRI    None      \n4                  quantity  decimal(16,2)  YES         None      \n5  cost_to_customer_per_qty  decimal(16,2)  YES         None      \n6          transaction_time           time   NO  PRI    None       \n\n****************************** market_date_info ******************************\n                Field          Type Null  Key Default Extra\n0         market_date          date   NO  PRI    None      \n1          market_day   varchar(45)  YES         None      \n2         market_week   varchar(45)  YES         None      \n3         market_year   varchar(45)  YES         None      \n4   market_start_time   varchar(45)  YES         None      \n5     market_end_time   varchar(45)  YES         None      \n6       special_notes          blob  YES         None      \n7       market_season   varchar(45)  YES         None      \n8     market_min_temp  varchar(200)  YES         None      \n9     market_max_temp   varchar(45)  YES         None      \n10   market_rain_flag       int(11)  YES         None      \n11   market_snow_flag       int(11)  YES         None       \n\n****************************** product ******************************\n                 Field         Type Null  Key Default Extra\n0           product_id      int(11)   NO  PRI    None      \n1         product_name  varchar(45)  YES         None      \n2         product_size  varchar(45)  YES         None      \n3  product_category_id      int(11)   NO  PRI    None      \n4     product_qty_type  varchar(45)  YES         None       \n\n****************************** product_category ******************************\n                   Field         Type Null  Key Default           Extra\n0    product_category_id      int(11)   NO  PRI    None  auto_increment\n1  product_category_name  varchar(45)  YES         None                 \n\n****************************** vendor ******************************\n                     Field         Type Null  Key Default           Extra\n0                vendor_id      int(11)   NO  PRI    None  auto_increment\n1              vendor_name  varchar(45)   NO  UNI    None                \n2              vendor_type  varchar(45)   NO         None                \n3  vendor_owner_first_name  varchar(45)   NO         None                \n4   vendor_owner_last_name  varchar(45)   NO         None                 \n\n****************************** vendor_booth_assignments ******************************\n          Field     Type Null  Key Default Extra\n0     vendor_id  int(11)   NO  PRI    None      \n1  booth_number  int(11)   NO  PRI    None      \n2   market_date     date   NO  PRI    None       \n\n****************************** vendor_inventory ******************************\n            Field           Type Null  Key Default Extra\n0     market_date           date   NO  PRI    None      \n1        quantity  decimal(16,2)  YES         None      \n2       vendor_id        int(11)   NO  PRI    None      \n3      product_id        int(11)   NO  PRI    None      \n4  original_price  decimal(16,2)  YES         None       \n\n****************************** zip_data ******************************\n                     Field     Type Null  Key Default Extra\n0               zip_code_5  char(5)   NO  PRI    None      \n1  median_household_income    float  YES         None      \n2      percent_high_income    float  YES         None      \n3         percent_under_18    float  YES         None      \n4          percent_over_65    float  YES         None      \n5       people_per_sq_mile    float  YES         None      \n6                 latitude    float  YES         None      \n7                longitude    float  YES         None       \n\n\n\n\nproduct = pd.read_sql('select * from product', db)\nproduct.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 23 entries, 0 to 22\nData columns (total 5 columns):\n #   Column               Non-Null Count  Dtype \n---  ------               --------------  ----- \n 0   product_id           23 non-null     int64 \n 1   product_name         23 non-null     object\n 2   product_size         22 non-null     object\n 3   product_category_id  23 non-null     int64 \n 4   product_qty_type     21 non-null     object\ndtypes: int64(2), object(3)\nmemory usage: 1.0+ KB\n\n\n\npd.read_sql('select product_id from product limit 5', db)\n\n\n\n\n\n  \n    \n      \n      product_id\n    \n  \n  \n    \n      0\n      1\n    \n    \n      1\n      2\n    \n    \n      2\n      3\n    \n    \n      3\n      9\n    \n    \n      4\n      12"
  },
  {
    "objectID": "posts/nbs/handle_missing_data.html",
    "href": "posts/nbs/handle_missing_data.html",
    "title": "Handling missing data",
    "section": "",
    "text": "In Pandas, there are 4 methods to handle NA values, which are dropna, fillna, isnull, notnull.\n\n\n\n\nflowchart LR\n  A([Handling NA]) --> B(dropna)\n  A --> C(fillna)\n  A --> D(isnull)\n  A --> E(notnull)\n  C --> F[ffill]\n  C --> G[bfill]\n\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\n\nstring_data = pd.Series(['apple', 'orange', np.nan, 'avocado'])\nstring_data\n\n0      apple\n1     orange\n2        NaN\n3    avocado\ndtype: object\n\n\n\nstring_data.isna()\n\n0    False\n1    False\n2     True\n3    False\ndtype: bool\n\n\n\nstring_data.isnull()\n\n0    False\n1    False\n2     True\n3    False\ndtype: bool\n\n\n\nstring_data[0] = None\n\n\nstring_data.isnull()\n\n0     True\n1    False\n2     True\n3    False\ndtype: bool\n\n\n\nstring_data.notnull()\n\n0    False\n1     True\n2    False\n3     True\ndtype: bool"
  },
  {
    "objectID": "posts/nbs/hf_datasets.html",
    "href": "posts/nbs/hf_datasets.html",
    "title": "Understanding Huggingface datasets",
    "section": "",
    "text": "from datasets import list_datasets\n\n\nall_datasets = list_datasets()\nlen(all_datasets), all_datasets[:10]\n\n(14199,\n ['acronym_identification',\n  'ade_corpus_v2',\n  'adversarial_qa',\n  'aeslc',\n  'afrikaans_ner_corpus',\n  'ag_news',\n  'ai2_arc',\n  'air_dialogue',\n  'ajgt_twitter_ar',\n  'allegro_reviews'])"
  },
  {
    "objectID": "posts/nbs/hf_datasets.html#load-a-dataset-from-huggingface",
    "href": "posts/nbs/hf_datasets.html#load-a-dataset-from-huggingface",
    "title": "Understanding Huggingface datasets",
    "section": "Load a dataset from huggingface",
    "text": "Load a dataset from huggingface\n\nfrom datasets import load_dataset\n\n\nLoad the whole dataset\n\nemotions = load_dataset('emotion')\nemotions\n\nUsing custom data configuration default\nReusing dataset emotion (/Users/youfeng/.cache/huggingface/datasets/emotion/default/0.0.0/348f63ca8e27b3713b6c04d723efe6d824a56fb3d1449794716c0f0296072705)\n\n\n\n\n\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 16000\n    })\n    validation: Dataset({\n        features: ['text', 'label'],\n        num_rows: 2000\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 2000\n    })\n})\n\n\n\n\nLoad part of the dataset\n\nemotions = load_dataset('emotion', split='train')\nemotions\n\nUsing custom data configuration default\nReusing dataset emotion (/Users/youfeng/.cache/huggingface/datasets/emotion/default/0.0.0/348f63ca8e27b3713b6c04d723efe6d824a56fb3d1449794716c0f0296072705)\n\n\nDataset({\n    features: ['text', 'label'],\n    num_rows: 16000\n})\n\n\n\nemotions = load_dataset('emotion', split=['validation', 'test'])\nemotions\n\nUsing custom data configuration default\nReusing dataset emotion (/Users/youfeng/.cache/huggingface/datasets/emotion/default/0.0.0/348f63ca8e27b3713b6c04d723efe6d824a56fb3d1449794716c0f0296072705)\n\n\n\n\n\n[Dataset({\n     features: ['text', 'label'],\n     num_rows: 2000\n }),\n Dataset({\n     features: ['text', 'label'],\n     num_rows: 2000\n })]"
  },
  {
    "objectID": "posts/nbs/hf_datasets.html#load-a-csv-file-from-remote-or-local",
    "href": "posts/nbs/hf_datasets.html#load-a-csv-file-from-remote-or-local",
    "title": "Understanding Huggingface datasets",
    "section": "Load a csv file from remote or local",
    "text": "Load a csv file from remote or local\n\nimport pandas as pd\nimport numpy as np\n\n\ntrn_csv = 'https://raw.githubusercontent.com/youfenglab/nlp-with-huggingface/master/data/train.csv'\ntst_csv = 'https://raw.githubusercontent.com/youfenglab/nlp-with-huggingface/master/data/train.csv'\n\n\ntrn_df = pd.read_csv(trn_csv)\nlabel_cols = trn_df.columns[2:]\nlabel_cols\n\nIndex(['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar',\n       'conventions'],\n      dtype='object')\n\n\n\ntrn_ds = load_dataset(\"csv\", data_files=trn_csv) # or several files []\ntrn_ds\n\nUsing custom data configuration default-8f5d5f2de1b27b24\nReusing dataset csv (/Users/youfeng/.cache/huggingface/datasets/csv/default-8f5d5f2de1b27b24/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n\n\n\n\n\nDatasetDict({\n    train: Dataset({\n        features: ['text_id', 'full_text', 'cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions'],\n        num_rows: 3911\n    })\n})\n\n\n\n\n\n\n\n\nTip: Load several files\n\n\n\ndata_files= [file1, file2 ...], but the columns of the files should be the same, otherwise you will get error.\n\n\nIf you want to split the training set into train and validation parts, you can do it as below\n\nds = trn_ds['train'].train_test_split(test_size=0.2) # Here we use 20% samples as validation part\nds\n\nDatasetDict({\n    train: Dataset({\n        features: ['text_id', 'full_text', 'cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions'],\n        num_rows: 3128\n    })\n    test: Dataset({\n        features: ['text_id', 'full_text', 'cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions'],\n        num_rows: 783\n    })\n})\n\n\nIf you want to use param ‚Äústratify‚Äù like scikit-learn, you can do it as below\n\n\n\n\n\n\nWarning\n\n\n\nNote: we need covert the column into ClassLabel column, since in this case the label columns are Value now. Otherwise, we will get an error like this:\nValueError: Stratifying by column is only supported for ClassLabel column, and column cohesion is Value.\n\n\nLet say we want use cohesion column.\n\ntrn_ds\n\nDatasetDict({\n    train: Dataset({\n        features: ['text_id', 'full_text', 'cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions'],\n        num_rows: 3911\n    })\n})\n\n\n\nds = trn_ds.class_encode_column('cohesion')\nds['train'].features\n\nLoading cached processed dataset at /Users/youfeng/.cache/huggingface/datasets/csv/default-8f5d5f2de1b27b24/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-8be23562eeb71b18.arrow\nLoading cached processed dataset at /Users/youfeng/.cache/huggingface/datasets/csv/default-8f5d5f2de1b27b24/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-b4dba551c432230d.arrow\nLoading cached processed dataset at /Users/youfeng/.cache/huggingface/datasets/csv/default-8f5d5f2de1b27b24/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-3e6fbdaa550cb114.arrow\n\n\n{'text_id': Value(dtype='string', id=None),\n 'full_text': Value(dtype='string', id=None),\n 'cohesion': ClassLabel(num_classes=9, names=['1.0', '1.5', '2.0', '2.5', '3.0', '3.5', '4.0', '4.5', '5.0'], id=None),\n 'syntax': Value(dtype='float64', id=None),\n 'vocabulary': Value(dtype='float64', id=None),\n 'phraseology': Value(dtype='float64', id=None),\n 'grammar': Value(dtype='float64', id=None),\n 'conventions': Value(dtype='float64', id=None)}\n\n\n\nds = ds['train'].train_test_split(test_size=0.2, stratify_by_column=\"cohesion\")\nds\n\nDatasetDict({\n    train: Dataset({\n        features: ['text_id', 'full_text', 'cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions'],\n        num_rows: 3128\n    })\n    test: Dataset({\n        features: ['text_id', 'full_text', 'cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions'],\n        num_rows: 783\n    })\n})"
  },
  {
    "objectID": "posts/nbs/activation_functions.html",
    "href": "posts/nbs/activation_functions.html",
    "title": "The common activation functions",
    "section": "",
    "text": "Activation functions decide whether a neuron should be activated or not. They are differentiable, then the information can be carried between the inputs and outputs. In practice, most of the actication functions are non-linearity.\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport torch"
  },
  {
    "objectID": "posts/nbs/activation_functions.html#the-common-activation-functions",
    "href": "posts/nbs/activation_functions.html#the-common-activation-functions",
    "title": "The common activation functions",
    "section": "The common activation functions",
    "text": "The common activation functions\n\nReLU function\n\\[ReLU(x) = max(x,0)\\]\n\nx = torch.arange(-10.0, 10, 0.1, requires_grad=True)\ny = torch.relu(x)\ny.backward(torch.ones_like(x), retain_graph=True)\n\n\nx.shape, x.grad.shape, y.shape\n\n(torch.Size([200]), torch.Size([200]), torch.Size([200]))\n\n\n\nfig, axs = plt.subplots(1, 2, figsize=(10, 3))\nflat_axs = axs.flatten()\n\nflat_axs[0].plot(x.detach(), y.detach())\nflat_axs[0].set_title('ReLU')\nflat_axs[0].set_xlabel('x')\nflat_axs[0].set_ylabel('y')\n\nflat_axs[1].plot(x.detach(), x.grad)\nflat_axs[1].set_title('grad of ReLU')\nflat_axs[1].set_xlabel('x')\nflat_axs[1].set_ylabel('grad of y')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\nSigmoid function\n\\[sigmoid(x) = \\frac{1} {1 + e^{-x}}\\]\n\nx.data.zero_()\nx = torch.arange(-10.0, 10, 0.1, requires_grad=True)\ny = torch.sigmoid(x)\ny.backward(torch.ones_like(x), retain_graph=True)\n\n\nfig, axs = plt.subplots(1, 2, figsize=(10, 3))\nflat_axs = axs.flatten()\n\nflat_axs[0].plot(x.detach(), y.detach())\nflat_axs[0].set_title('Sigmoid')\nflat_axs[0].set_xlabel('x')\nflat_axs[0].set_ylabel('y')\n\nflat_axs[1].plot(x.detach(), x.grad)\nflat_axs[1].set_title('grad of Sigmoid')\nflat_axs[1].set_xlabel('x')\nflat_axs[1].set_ylabel('grad of y')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\nTanh function\n\\[tanh(x) = \\frac{1 - e^{-2x}} {1 + e^{-2x}}\\]\n\nx.data.zero_()\nx = torch.arange(-10.0, 10, 0.1, requires_grad=True)\ny = torch.tanh(x)\ny.backward(torch.ones_like(x), retain_graph=True)\n\n\nfig, axs = plt.subplots(1, 2, figsize=(10, 3))\nflat_axs = axs.flatten()\n\nflat_axs[0].plot(x.detach(), y.detach())\nflat_axs[0].set_title('Tanh')\nflat_axs[0].set_xlabel('x')\nflat_axs[0].set_ylabel('y')\n\nflat_axs[1].plot(x.detach(), x.grad)\nflat_axs[1].set_title('grad of Tanh')\nflat_axs[1].set_xlabel('x')\nflat_axs[1].set_ylabel('grad of y')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Posts",
    "section": "",
    "text": "The common activation functions\n\n\n\n\n\n\n\ndeep learning\n\n\nmath\n\n\n\n\nBasic math for deep learning\n\n\n\n\n\n\nNov 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHandling missing data\n\n\n\n\n\n\n\npython\n\n\npandas\n\n\nnumpy\n\n\n\n\nExamples of dealing with missing data\n\n\n\n\n\n\nOct 30, 2022\n\n\n\n\n\n\n  \n\n\n\n\nUnderstanding Huggingface datasets\n\n\n\n\n\n\n\nnlp\n\n\nhuggingface\n\n\n\n\nThis blog will dive into huggingface datasets\n\n\n\n\n\n\nSep 19, 2022\n\n\n\n\n\n\n  \n\n\n\n\nConnect to MySql with Pandas\n\n\n\n\n\n\n\nmysql\n\n\ndatabase\n\n\npandas\n\n\n\n\nThis blog show how to connect to MySql database via Pandas\n\n\n\n\n\n\nAug 19, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My latest projects",
    "section": "",
    "text": "My latest projects\n\n\n\n\n\n\n\n\n\n\nExtracting and grouping text info by category\n\n\n\ndata cleaning\n\n\nregex\n\n\n\nThis notebook shows matching text pattern and extract text and processing strings using Regex.\n\n\n\nOct 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGenerate English name\n\n\n\ndeep learning\n\n\nnlp\n\n\n\nTrain a language model to generate next character based on the previous characters to output an english name.\n\n\n\nSep 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEigenFace\n\n\n\nPCA\n\n\nSVD\n\n\n\nThis notebook will dive into eigenvalues and eigenvectors with covariance matrix. We also compare this method with SVD to understand what happens under the hood.\n\n\n\nMay 26, 2022\n\n\n\n\n\n\n\n\nNo matching items\n\n\n More projects\n\n\n\nMy latest posts\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe common activation functions\n\n\n\n\n\n\n\ndeep learning\n\n\nmath\n\n\n\n\nBasic math for deep learning\n\n\n\n\n\n\nNov 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHandling missing data\n\n\n\n\n\n\n\npython\n\n\npandas\n\n\nnumpy\n\n\n\n\nExamples of dealing with missing data\n\n\n\n\n\n\nOct 30, 2022\n\n\n\n\n\n\n  \n\n\n\n\nUnderstanding Huggingface datasets\n\n\n\n\n\n\n\nnlp\n\n\nhuggingface\n\n\n\n\nThis blog will dive into huggingface datasets\n\n\n\n\n\n\nSep 19, 2022\n\n\n\n\n\n\n  \n\n\n\n\nConnect to MySql with Pandas\n\n\n\n\n\n\n\nmysql\n\n\ndatabase\n\n\npandas\n\n\n\n\nThis blog show how to connect to MySql database via Pandas\n\n\n\n\n\n\nAug 19, 2022\n\n\n\n\n\n\nNo matching items\n\n\n More posts"
  },
  {
    "objectID": "tils/nbs/define-a-spark-schema.html",
    "href": "tils/nbs/define-a-spark-schema.html",
    "title": "Define a spark schema",
    "section": "",
    "text": "from pyspark.sql.types import *\nfrom pyspark.sql import SparkSession\n\n\nspark = SparkSession.builder.appName('Simple-table').getOrCreate()"
  },
  {
    "objectID": "tils/nbs/define-a-spark-schema.html#define-it-programmatically",
    "href": "tils/nbs/define-a-spark-schema.html#define-it-programmatically",
    "title": "Define a spark schema",
    "section": "Define it programmatically",
    "text": "Define it programmatically\n\nschema = StructType([StructField(\"first name\", StringType(), False), \n                     StructField(\"last name\", StringType(), False), \n                     StructField(\"weight\", IntegerType(), False)])\n\n\nschema\n\nStructType([StructField('first name', StringType(), False), StructField('last name', StringType(), False), StructField('weight', IntegerType(), False)])\n\n\nFalse indicate whether the field can be null (None) or not.\n\ndata = [['Jake', 'Z', 60], ['Tom', 'X', 50]]\n\n\ndf = spark.createDataFrame(data, schema)\ndf.show()\n\n+----------+---------+------+\n|first name|last name|weight|\n+----------+---------+------+\n|      Jake|        Z|    60|\n|       Tom|        X|    50|\n+----------+---------+------+"
  },
  {
    "objectID": "tils/nbs/define-a-spark-schema.html#define-it-using-ddl",
    "href": "tils/nbs/define-a-spark-schema.html#define-it-using-ddl",
    "title": "Define a spark schema",
    "section": "Define it using DDL",
    "text": "Define it using DDL\nThis method is much simper.\n\nschema = \"first_name STRING, last_name STRING, weight INT\"\n\n\nschema\n\n'first_name STRING, last_name STRING, weight INT'\n\n\n\ndf = spark.createDataFrame(data, schema)\ndf.show()\n\n+----------+---------+------+\n|first_name|last_name|weight|\n+----------+---------+------+\n|      Jake|        Z|    60|\n|       Tom|        X|    50|\n+----------+---------+------+\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nA disadvantage of this method is we cannot put space or - between words."
  },
  {
    "objectID": "tils/nbs/define-a-spark-schema.html#stop-spark-session",
    "href": "tils/nbs/define-a-spark-schema.html#stop-spark-session",
    "title": "Define a spark schema",
    "section": "Stop spark session",
    "text": "Stop spark session\n\nspark.stop()"
  },
  {
    "objectID": "tils/nbs/expressivity-and-composability-of-spark.html",
    "href": "tils/nbs/expressivity-and-composability-of-spark.html",
    "title": "Expressivity and composability of Spark",
    "section": "",
    "text": "from pyspark import SparkContext\n\nsc = SparkContext('local')\n\n\ndataRDD = sc.parallelize([('Brooke', 20), ('Denny', 31), ('Jules', 30), ('TD', 35), ('Brooke', 25)])\n\n\ndataRDD.first()\n\n('Brooke', 20)\n\n\n\ndataRDD.collect() # only use when the dataset is small\n\n[('Brooke', 20), ('Denny', 31), ('Jules', 30), ('TD', 35), ('Brooke', 25)]\n\n\n\ndataRDD.take(2) # use `take(n)` to get the first n rows\n\n[('Brooke', 20), ('Denny', 31)]\n\n\n\nmapedRDD = dataRDD.map(lambda x: (x[0], (x[1], 1)))\nmapedRDD.collect()\n\n[('Brooke', (20, 1)),\n ('Denny', (31, 1)),\n ('Jules', (30, 1)),\n ('TD', (35, 1)),\n ('Brooke', (25, 1))]\n\n\n\nreducedRDD = mapedRDD.reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\nreducedRDD.collect()\n\n[('Brooke', (45, 2)), ('Denny', (31, 1)), ('Jules', (30, 1)), ('TD', (35, 1))]\n\n\n\nmapedRDD = reducedRDD.map(lambda x: (x[0], x[1][0]/x[1][1]))\nmapedRDD.collect()\n\n[('Brooke', 22.5), ('Denny', 31.0), ('Jules', 30.0), ('TD', 35.0)]\n\n\n\nsc.stop()"
  },
  {
    "objectID": "tils/nbs/expressivity-and-composability-of-spark.html#with-high-level-dsl-operates-and-dataframe-api-structured",
    "href": "tils/nbs/expressivity-and-composability-of-spark.html#with-high-level-dsl-operates-and-dataframe-api-structured",
    "title": "Expressivity and composability of Spark",
    "section": "With high-level DSL operates and DataFrame API (structured)",
    "text": "With high-level DSL operates and DataFrame API (structured)\n\nDSL: domain specific language\n\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import avg\n\nspark = SparkSession.builder.appName('AvgAges').getOrCreate()\n\n\ndata_df = spark.createDataFrame([('Brooke', 20), ('Denny', 31), ('Jules', 30), ('TD', 35), ('Brooke', 25)], \n                                ['name', 'age'])\ndata_df.show()\n\n[Stage 0:>                                                          (0 + 1) / 1]                                                                                \n\n\n+------+---+\n|  name|age|\n+------+---+\n|Brooke| 20|\n| Denny| 31|\n| Jules| 30|\n|    TD| 35|\n|Brooke| 25|\n+------+---+\n\n\n\n\ndata_df.show(1)\n\n+------+---+\n|  name|age|\n+------+---+\n|Brooke| 20|\n+------+---+\nonly showing top 1 row\n\n\n\n\ndata_df.take(2) # Get the first 2 rows\n\n[Row(name='Brooke', age=20), Row(name='Denny', age=31)]\n\n\n\ndata_df.tail(2) # Get the last 3 rows\n\n[Row(name='TD', age=35), Row(name='Brooke', age=25)]\n\n\n\navg_df = data_df.groupBy('name').agg(avg('age'))\navg_df.show()\n\n+------+--------+\n|  name|avg(age)|\n+------+--------+\n|Brooke|    22.5|\n| Denny|    31.0|\n| Jules|    30.0|\n|    TD|    35.0|\n+------+--------+\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis version is far more expressive and simpler than the previous one.\n\n\n\nspark.stop()"
  },
  {
    "objectID": "tils/nbs/common-dataframe-operations.html",
    "href": "tils/nbs/common-dataframe-operations.html",
    "title": "Common DataFrame operations",
    "section": "",
    "text": "Tip\n\n\n\nIf we don‚Äôt specify the schema, Spark can infer the schema from the data.\nHowever, for large datasets and files, it‚Äôs more efficient to define a schema than have Spark infer it.\n\n\n\n\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import *\n\nspark = SparkSession.builder.appName('CSV-Reader').getOrCreate()\n\n# In python, the syntax is as below.\ndf = spark.read.csv('csv_file_path', header=True, schema=csv_file_schema)\n\n\n\n\n\n\nImportant\n\n\n\ncsv_file_schema is defined before reading the file.\n\n\n\n\n\n\nSave as a parquet fileSave as a parquet table\n\n\nparquet_path = '...'\ndf.write.format(\"parquet\").save(parquet_path)\n\n\nparquet_table = '...'                                   # name of the table\ndf.write.format(\"parquet\").saveAsTable(parquet_table)\n\n\n\n\nSQL table will cover later\n\n\n\n\n# in Spark, projections are done with `select()` method,\n# while filters can be conducted using `filter()` or `where()` method.\nsub_df = df.select([columns list]).where(col(column name == 'some condition'))\nsub_df.show(5, truncate=False)\n\n\n\n\nRenamingAddingDropping\n\n\ndf.withColumnRenamed('name of current column', 'renamed column name')\n\n\ndf.withColumn('target column', 'new_column')\n\n\ndf.drop('columns needed to drop')\n\n\n\nMore details look at book Learning Spark\n\nspark.stop()"
  },
  {
    "objectID": "tils/nbs/the-useage-of-shutil-module.html",
    "href": "tils/nbs/the-useage-of-shutil-module.html",
    "title": "The useage of python shutil module",
    "section": "",
    "text": "shutil.copy(src, dst)\n\n\n\nshutil.copy2(src, dst)\n\n\n\nshutil.copytree(src, dst)\n\n\n\nshutil.move(src, dst)"
  },
  {
    "objectID": "tils/nbs/the-useage-of-shutil-module.html#create-and-unpack-archives",
    "href": "tils/nbs/the-useage-of-shutil-module.html#create-and-unpack-archives",
    "title": "The useage of python shutil module",
    "section": "Create and unpack archives",
    "text": "Create and unpack archives\n\nCreate archives\nshutil.make_archive(archive_name,\n                    'zip',  # the desired output format\n                    root_dir)\n\n# Get a list of supported archive formats\nshutil.get_archive_formats()\n\n[('bztar', \"bzip2'ed tar-file\"),\n ('gztar', \"gzip'ed tar-file\"),\n ('tar', 'uncompressed tar file'),\n ('xztar', \"xz'ed tar-file\"),\n ('zip', 'ZIP file')]\n\n\n\n\nunpack archives\nshutil.unpack_archive(archive_name, \n                      extract_dir)"
  },
  {
    "objectID": "tils/nbs/find-path-of-a-linux-command.html",
    "href": "tils/nbs/find-path-of-a-linux-command.html",
    "title": "Find the path of a Linux command",
    "section": "",
    "text": "Suppose we want to find the path of the command spark-submit in terminal, we have 4 ways to do it."
  },
  {
    "objectID": "tils/nbs/find-path-of-a-linux-command.html#which-command",
    "href": "tils/nbs/find-path-of-a-linux-command.html#which-command",
    "title": "Find the path of a Linux command",
    "section": "1. which command",
    "text": "1. which command\n\n!which spark-submit\n\n/Users/youfeng/mambaforge/bin/spark-submit\n\n\nWith a parameter -a will print all matching paths. In this case, there is only one.\n\n!which -a spark-submit\n\n/Users/youfeng/mambaforge/bin/spark-submit"
  },
  {
    "objectID": "tils/nbs/find-path-of-a-linux-command.html#command-command",
    "href": "tils/nbs/find-path-of-a-linux-command.html#command-command",
    "title": "Find the path of a Linux command",
    "section": "2. command command",
    "text": "2. command command\n\n!command -v spark-submit\n\n/Users/youfeng/mambaforge/bin/spark-submit\n\n\n\n!command -V spark-submit\n\nspark-submit is /Users/youfeng/mambaforge/bin/spark-submit\n\n\n\n\n\n\n\n\nWarning\n\n\n\nWe must pass the parameter -v or -V, otherwise, it will run the command."
  },
  {
    "objectID": "tils/nbs/find-path-of-a-linux-command.html#type-command",
    "href": "tils/nbs/find-path-of-a-linux-command.html#type-command",
    "title": "Find the path of a Linux command",
    "section": "3. type command",
    "text": "3. type command\nShow the path\n\n!type -p spark-submit\n\nspark-submit is /Users/youfeng/mambaforge/bin/spark-submit\n\n\nShow the definition\n\n!type spark-submit\n\nspark-submit is /Users/youfeng/mambaforge/bin/spark-submit\n\n\nShow the definition, excutable type, and path\n\n!type -a spark-submit\n\nspark-submit is /Users/youfeng/mambaforge/bin/spark-submit"
  },
  {
    "objectID": "tils/nbs/find-path-of-a-linux-command.html#whereis-command",
    "href": "tils/nbs/find-path-of-a-linux-command.html#whereis-command",
    "title": "Find the path of a Linux command",
    "section": "4. whereis command",
    "text": "4. whereis command\nShow all the locations of the binary, source, manual page\n\n!whereis spark-submit\n\nspark-submit: /Users/youfeng/mambaforge/bin/spark-submit\n\n\nShow the locations of the binary\n\n!whereis -b spark-submit\n\nspark-submit: /Users/youfeng/mambaforge/bin/spark-submit\n\n\nShow the locations of the source"
  },
  {
    "objectID": "tils/nbs/column-and-row-in-spark.html",
    "href": "tils/nbs/column-and-row-in-spark.html",
    "title": "Row and column in Spark",
    "section": "",
    "text": "from pyspark.sql import SparkSession\nfrom pyspark.sql import Row, Column\nfrom pyspark.sql.functions import expr\n\nspark = SparkSession.builder.appName('col-row').getOrCreate()\n\n\nrows = [Row('Brooke', 20), Row('Denny', 31), Row('Jules', 30), Row('TD', 35), Row('Brooke', 25)]\nrows\n\n[<Row('Brooke', 20)>,\n <Row('Denny', 31)>,\n <Row('Jules', 30)>,\n <Row('TD', 35)>,\n <Row('Brooke', 25)>]\n\n\n\ndf = spark.createDataFrame(rows, ['name', 'age'])\ndf.show()\n\n[Stage 0:>                                                          (0 + 1) / 1]\n\n\n+------+---+\n|  name|age|\n+------+---+\n|Brooke| 20|\n| Denny| 31|\n| Jules| 30|\n|    TD| 35|\n|Brooke| 25|\n+------+---+"
  },
  {
    "objectID": "tils/nbs/column-and-row-in-spark.html#column",
    "href": "tils/nbs/column-and-row-in-spark.html#column",
    "title": "Row and column in Spark",
    "section": "Column",
    "text": "Column\n\nnew_col = expr(\"age + 3\")\nnew_col\n\nColumn<'(age + 3)'>\n\n\n\ndf.withColumn('age after 3 years', new_col).show()\n\n+------+---+-----------------+\n|  name|age|age after 3 years|\n+------+---+-----------------+\n|Brooke| 20|               23|\n| Denny| 31|               34|\n| Jules| 30|               33|\n|    TD| 35|               38|\n|Brooke| 25|               28|\n+------+---+-----------------+\n\n\n\n\ndf.age\n\nColumn<'age'>\n\n\n\ndf.withColumn('age after 5 years', (df.age + 5)).show()\n\n+------+---+-----------------+\n|  name|age|age after 5 years|\n+------+---+-----------------+\n|Brooke| 20|               25|\n| Denny| 31|               36|\n| Jules| 30|               35|\n|    TD| 35|               40|\n|Brooke| 25|               30|\n+------+---+-----------------+\n\n\n\n\nspark.stop()"
  },
  {
    "objectID": "tils/nbs/setup-spark-in-jupyter-notebook.html",
    "href": "tils/nbs/setup-spark-in-jupyter-notebook.html",
    "title": "Set up spark in Jupyter Notebook",
    "section": "",
    "text": "from pyspark import SparkContext, SparkConf\n\nconf = SparkConf().setMaster('local')\nsc = SparkContext(conf=conf)\n\n\nsc.stop()"
  },
  {
    "objectID": "tils/nbs/setup-spark-in-jupyter-notebook.html#using-high-level-dsl-operators-and-the-dataframe-api",
    "href": "tils/nbs/setup-spark-in-jupyter-notebook.html#using-high-level-dsl-operators-and-the-dataframe-api",
    "title": "Set up spark in Jupyter Notebook",
    "section": "Using high-level DSL operators and the DataFrame API",
    "text": "Using high-level DSL operators and the DataFrame API\n\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName('chapter2').getOrCreate()\n\n\nspark.stop()"
  },
  {
    "objectID": "tils/nbs/image-data-augmentation.html",
    "href": "tils/nbs/image-data-augmentation.html",
    "title": "Image data augmentation",
    "section": "",
    "text": "This notebook will show the common methods used for preparing image data for vision models in PyTorch."
  },
  {
    "objectID": "tils/nbs/image-data-augmentation.html#load-an-original-image",
    "href": "tils/nbs/image-data-augmentation.html#load-an-original-image",
    "title": "Image data augmentation",
    "section": "Load an original image",
    "text": "Load an original image\n\nimg = Image.open('images/cat.jpeg')\nimg"
  },
  {
    "objectID": "tils/nbs/image-data-augmentation.html#convert-the-image-to-tensor",
    "href": "tils/nbs/image-data-augmentation.html#convert-the-image-to-tensor",
    "title": "Image data augmentation",
    "section": "Convert the image to Tensor",
    "text": "Convert the image to Tensor\n\ntoTensor = torchvision.transforms.ToTensor()\ntoTensor(img).shape\n\ntorch.Size([3, 1199, 1200])\n\n\nFrom above, we know that the image size is 1199 * 1200."
  },
  {
    "objectID": "tils/nbs/image-data-augmentation.html#resize-the-image",
    "href": "tils/nbs/image-data-augmentation.html#resize-the-image",
    "title": "Image data augmentation",
    "section": "Resize the image",
    "text": "Resize the image\nHere we resize the image to size 224 \\(\\times\\) 224.\n\nresize = torchvision.transforms.Resize(224)\nimg_rs = resize(img)\nimg_rs"
  },
  {
    "objectID": "tils/nbs/image-data-augmentation.html#flip-an-image",
    "href": "tils/nbs/image-data-augmentation.html#flip-an-image",
    "title": "Image data augmentation",
    "section": "Flip an image",
    "text": "Flip an image\n\n1. Flip horizontally\n\nflip = torchvision.transforms.RandomHorizontalFlip(p=1.0)\nflip(img_rs)\n\n\n\n\n\n\n2. Flip vertically\n\nflip = torchvision.transforms.RandomVerticalFlip(p=1.0)\nflip(img_rs)"
  },
  {
    "objectID": "tils/nbs/image-data-augmentation.html#change-brightness-contrast-saturation-and-hue-of-an-image",
    "href": "tils/nbs/image-data-augmentation.html#change-brightness-contrast-saturation-and-hue-of-an-image",
    "title": "Image data augmentation",
    "section": "Change brightness, contrast, saturation and hue of an image",
    "text": "Change brightness, contrast, saturation and hue of an image\n\ncolorjitter = torchvision.transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.3)\ncolorjitter(img_rs)"
  },
  {
    "objectID": "tils/nbs/image-data-augmentation.html#turn-an-image-grayscale",
    "href": "tils/nbs/image-data-augmentation.html#turn-an-image-grayscale",
    "title": "Image data augmentation",
    "section": "Turn an image grayscale",
    "text": "Turn an image grayscale\n\ngrayscale = torchvision.transforms.Grayscale()\ngrayscale(img_rs)"
  },
  {
    "objectID": "tils/nbs/image-data-augmentation.html#crop-an-image",
    "href": "tils/nbs/image-data-augmentation.html#crop-an-image",
    "title": "Image data augmentation",
    "section": "Crop an image",
    "text": "Crop an image\n\ncrop = torchvision.transforms.CenterCrop(128)\ncrop(img_rs)"
  },
  {
    "objectID": "tils/index.html",
    "href": "tils/index.html",
    "title": "TILs",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\n\n\n\n\nNov 22, 2022\n\n\nThe useage of python shutil module\n\n\n\n\nNov 21, 2022\n\n\nExpressivity and composability of Spark\n\n\n\n\nNov 21, 2022\n\n\nCommon DataFrame operations\n\n\n\n\nNov 21, 2022\n\n\nRow and column in Spark\n\n\n\n\nNov 17, 2022\n\n\nDefine a spark schema\n\n\n\n\nNov 15, 2022\n\n\nFind the path of a Linux command\n\n\n\n\nNov 15, 2022\n\n\nSet up spark in Jupyter Notebook\n\n\n\n\nNov 10, 2022\n\n\nImage data augmentation\n\n\n\n\n\n\nNo matching items"
  }
]