[
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Posts",
    "section": "",
    "text": "My posts\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe common activation functions\n\n\n\n\n\n\n\ndeep learning\n\n\nmath\n\n\n\n\nBasic math for deep learning\n\n\n\n\n\n\nNov 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHandling missing data\n\n\n\n\n\n\n\npython\n\n\npandas\n\n\nnumpy\n\n\n\n\nExamples of dealing with missing data\n\n\n\n\n\n\nOct 30, 2022\n\n\n\n\n\n\n  \n\n\n\n\nConnect to MySql with Pandas\n\n\n\n\n\n\n\nmysql\n\n\ndatabase\n\n\npandas\n\n\n\n\nThis blog show how to connect to MySql database via Pandas\n\n\n\n\n\n\nSep 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Huggingface datasets\n\n\n\n\n\n\n\nnlp\n\n\nhuggingface\n\n\n\n\nThis blog will dive into huggingface datasets\n\n\n\n\n\n\nAug 19, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/nbs/activation_functions.html",
    "href": "posts/nbs/activation_functions.html",
    "title": "The common activation functions",
    "section": "",
    "text": "Why we need activation functions?\nActivation functions decide whether a neuron should be activated or not. They are differentiable, then the information can be carried between the inputs and outputs. In practice, most of the actication functions are non-linearity.\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport torch\n\n\n\nReLU function\n\\[ReLU(x) = max(x,0)\\]\n\nx = torch.arange(-10.0, 10, 0.1, requires_grad=True)\ny = torch.relu(x)\ny.backward(torch.ones_like(x), retain_graph=True)\n\n\nx.shape, x.grad.shape, y.shape\n\n(torch.Size([200]), torch.Size([200]), torch.Size([200]))\n\n\n\nfig, axs = plt.subplots(1, 2, figsize=(10, 3))\nflat_axs = axs.flatten()\n\nflat_axs[0].plot(x.detach(), y.detach())\nflat_axs[0].set_title('ReLU')\nflat_axs[0].set_xlabel('x')\nflat_axs[0].set_ylabel('y')\n\nflat_axs[1].plot(x.detach(), x.grad)\nflat_axs[1].set_title('grad of ReLU')\nflat_axs[1].set_xlabel('x')\nflat_axs[1].set_ylabel('grad of y')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\nSigmoid function\n\\[sigmoid(x) = \\frac{1} {1 + e^{-x}}\\]\n\nx.data.zero_()\nx = torch.arange(-10.0, 10, 0.1, requires_grad=True)\ny = torch.sigmoid(x)\ny.backward(torch.ones_like(x), retain_graph=True)\n\n\nfig, axs = plt.subplots(1, 2, figsize=(10, 3))\nflat_axs = axs.flatten()\n\nflat_axs[0].plot(x.detach(), y.detach())\nflat_axs[0].set_title('Sigmoid')\nflat_axs[0].set_xlabel('x')\nflat_axs[0].set_ylabel('y')\n\nflat_axs[1].plot(x.detach(), x.grad)\nflat_axs[1].set_title('grad of Sigmoid')\nflat_axs[1].set_xlabel('x')\nflat_axs[1].set_ylabel('grad of y')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\nTanh function\n\\[tanh(x) = \\frac{1 - e^{-2x}} {1 + e^{-2x}}\\]\n\nx.data.zero_()\nx = torch.arange(-10.0, 10, 0.1, requires_grad=True)\ny = torch.tanh(x)\ny.backward(torch.ones_like(x), retain_graph=True)\n\n\nfig, axs = plt.subplots(1, 2, figsize=(10, 3))\nflat_axs = axs.flatten()\n\nflat_axs[0].plot(x.detach(), y.detach())\nflat_axs[0].set_title('Tanh')\nflat_axs[0].set_xlabel('x')\nflat_axs[0].set_ylabel('y')\n\nflat_axs[1].plot(x.detach(), x.grad)\nflat_axs[1].set_title('grad of Tanh')\nflat_axs[1].set_xlabel('x')\nflat_axs[1].set_ylabel('grad of y')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/nbs/handle_missing_data.html",
    "href": "posts/nbs/handle_missing_data.html",
    "title": "Handling missing data",
    "section": "",
    "text": "In Pandas, there are 4 methods to handle NA values, which are dropna, fillna, isnull, notnull.\n\n\n\n\nflowchart LR\n  A([Handling NA]) --> B(dropna)\n  A --> C(fillna)\n  A --> D(isnull)\n  A --> E(notnull)\n  C --> F[ffill]\n  C --> G[bfill]\n\n\n\n\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\n\nstring_data = pd.Series(['apple', 'orange', np.nan, 'avocado'])\nstring_data\n\n0      apple\n1     orange\n2        NaN\n3    avocado\ndtype: object\n\n\n\nstring_data.isna()\n\n0    False\n1    False\n2     True\n3    False\ndtype: bool\n\n\n\nstring_data.isnull()\n\n0    False\n1    False\n2     True\n3    False\ndtype: bool\n\n\n\nstring_data[0] = None\n\n\nstring_data.isnull()\n\n0     True\n1    False\n2     True\n3    False\ndtype: bool\n\n\n\nstring_data.notnull()\n\n0    False\n1     True\n2    False\n3     True\ndtype: bool"
  },
  {
    "objectID": "posts/nbs/hf_datasets.html",
    "href": "posts/nbs/hf_datasets.html",
    "title": "Understanding Huggingface datasets",
    "section": "",
    "text": "Get the list of all avaliable datasets from haggingface\n\nfrom datasets import list_datasets\n\n\nall_datasets = list_datasets()\nlen(all_datasets), all_datasets[:10]\n\n(12223,\n ['acronym_identification',\n  'ade_corpus_v2',\n  'adversarial_qa',\n  'aeslc',\n  'afrikaans_ner_corpus',\n  'ag_news',\n  'ai2_arc',\n  'air_dialogue',\n  'ajgt_twitter_ar',\n  'allegro_reviews'])\n\n\n\n\nLoad a dataset from huggingface\n\nfrom datasets import load_dataset\n\n1.Load the whole dataset\n\nemotions = load_dataset('emotion')\nemotions\n\nUsing custom data configuration default\nReusing dataset emotion (/Users/youfeng/.cache/huggingface/datasets/emotion/default/0.0.0/348f63ca8e27b3713b6c04d723efe6d824a56fb3d1449794716c0f0296072705)\n\n\n\n\n\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 16000\n    })\n    validation: Dataset({\n        features: ['text', 'label'],\n        num_rows: 2000\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 2000\n    })\n})\n\n\n2.Load part of the dataset\n\nemotions = load_dataset('emotion', split='train')\nemotions\n\nUsing custom data configuration default\nReusing dataset emotion (/Users/youfeng/.cache/huggingface/datasets/emotion/default/0.0.0/348f63ca8e27b3713b6c04d723efe6d824a56fb3d1449794716c0f0296072705)\n\n\nDataset({\n    features: ['text', 'label'],\n    num_rows: 16000\n})\n\n\n\nemotions = load_dataset('emotion', split=['validation', 'test'])\nemotions\n\nUsing custom data configuration default\nReusing dataset emotion (/Users/youfeng/.cache/huggingface/datasets/emotion/default/0.0.0/348f63ca8e27b3713b6c04d723efe6d824a56fb3d1449794716c0f0296072705)\n\n\n\n\n\n[Dataset({\n     features: ['text', 'label'],\n     num_rows: 2000\n }),\n Dataset({\n     features: ['text', 'label'],\n     num_rows: 2000\n })]\n\n\n\n\nLoad a csv file from remote or local\n\nimport pandas as pd\nimport numpy as np\n\n\ntrn_csv = 'https://raw.githubusercontent.com/youfenglab/nlp-with-huggingface/master/data/train.csv'\ntst_csv = 'https://raw.githubusercontent.com/youfenglab/nlp-with-huggingface/master/data/train.csv'\n\n\ntrn_df = pd.read_csv(trn_csv)\nlabel_cols = trn_df.columns[2:]\nlabel_cols\n\nIndex(['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar',\n       'conventions'],\n      dtype='object')\n\n\n\ntrn_ds = load_dataset(\"csv\", data_files=trn_csv) # or several files []\ntrn_ds\n\nUsing custom data configuration default-8f5d5f2de1b27b24\nReusing dataset csv (/Users/youfeng/.cache/huggingface/datasets/csv/default-8f5d5f2de1b27b24/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n\n\n\n\n\nDatasetDict({\n    train: Dataset({\n        features: ['text_id', 'full_text', 'cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions'],\n        num_rows: 3911\n    })\n})\n\n\n\n\n\n\n\n\nTip: Load several files\n\n\n\ndata_files= [file1, file2 ...], but the columns of the files should be the same, otherwise you will get error.\n\n\nIf you want to split the training set into train and validation parts, you can do it as below\n\nds = trn_ds['train'].train_test_split(test_size=0.2) # Here we use 20% samples as validation part\nds\n\nDatasetDict({\n    train: Dataset({\n        features: ['text_id', 'full_text', 'cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions'],\n        num_rows: 3128\n    })\n    test: Dataset({\n        features: ['text_id', 'full_text', 'cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions'],\n        num_rows: 783\n    })\n})\n\n\nIf you want to use param “stratify” like scikit-learn, you can do it as below\n\n\n\n\n\n\nWarning\n\n\n\nNote: we need covert the column into ClassLabel column, since in this case the label columns are Value now. Otherwise, we will get an error like this:\nValueError: Stratifying by column is only supported for ClassLabel column, and column cohesion is Value.\n\n\nLet say we want use cohesion column.\n\ntrn_ds\n\nDatasetDict({\n    train: Dataset({\n        features: ['text_id', 'full_text', 'cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions'],\n        num_rows: 3911\n    })\n})\n\n\n\nds = trn_ds.class_encode_column('cohesion')\nds['train'].features\n\nLoading cached processed dataset at /Users/youfeng/.cache/huggingface/datasets/csv/default-8f5d5f2de1b27b24/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-8be23562eeb71b18.arrow\nLoading cached processed dataset at /Users/youfeng/.cache/huggingface/datasets/csv/default-8f5d5f2de1b27b24/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-b4dba551c432230d.arrow\nLoading cached processed dataset at /Users/youfeng/.cache/huggingface/datasets/csv/default-8f5d5f2de1b27b24/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-3e6fbdaa550cb114.arrow\n\n\n{'text_id': Value(dtype='string', id=None),\n 'full_text': Value(dtype='string', id=None),\n 'cohesion': ClassLabel(num_classes=9, names=['1.0', '1.5', '2.0', '2.5', '3.0', '3.5', '4.0', '4.5', '5.0'], id=None),\n 'syntax': Value(dtype='float64', id=None),\n 'vocabulary': Value(dtype='float64', id=None),\n 'phraseology': Value(dtype='float64', id=None),\n 'grammar': Value(dtype='float64', id=None),\n 'conventions': Value(dtype='float64', id=None)}\n\n\n\nds = ds['train'].train_test_split(test_size=0.2, stratify_by_column=\"cohesion\")\nds\n\nDatasetDict({\n    train: Dataset({\n        features: ['text_id', 'full_text', 'cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions'],\n        num_rows: 3128\n    })\n    test: Dataset({\n        features: ['text_id', 'full_text', 'cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions'],\n        num_rows: 783\n    })\n})"
  },
  {
    "objectID": "posts/nbs/connect_db.html",
    "href": "posts/nbs/connect_db.html",
    "title": "Connect to MySql with Pandas",
    "section": "",
    "text": "Import packages\n\nimport sqlalchemy as sqla\nimport pandas as pd\n\n\n\nConnect to a database\n\ndb = sqla.create_engine('mysql+mysqlconnector://root:@localhost:3306/farmers_market')\n\nGet all the tables in the database\n\ndb_tables = pd.read_sql('show tables from farmers_market', db)\ndb_tables\n\n\n\n\n\n  \n    \n      \n      Tables_in_farmers_market\n    \n  \n  \n    \n      0\n      booth\n    \n    \n      1\n      customer\n    \n    \n      2\n      customer_purchases\n    \n    \n      3\n      market_date_info\n    \n    \n      4\n      product\n    \n    \n      5\n      product_category\n    \n    \n      6\n      vendor\n    \n    \n      7\n      vendor_booth_assignments\n    \n    \n      8\n      vendor_inventory\n    \n    \n      9\n      zip_data\n    \n  \n\n\n\n\n\n\nGet information about the schema of each table\n\ntables = db_tables['Tables_in_farmers_market']\n\nfor table in tables:\n    desc = pd.read_sql('describe {}'.format(table), db)\n    print('*'*30, table, '*'*30)\n    print(desc, '\\n')\n\n****************************** booth ******************************\n               Field          Type Null  Key Default Extra\n0       booth_number       int(11)   NO  PRI    None      \n1  booth_price_level   varchar(45)   NO         None      \n2  booth_description  varchar(255)   NO         None      \n3         booth_type   varchar(45)   NO         None       \n\n****************************** customer ******************************\n                 Field         Type Null  Key Default Extra\n0          customer_id      int(11)   NO  PRI    None      \n1  customer_first_name  varchar(45)  YES         None      \n2   customer_last_name  varchar(45)  YES         None      \n3         customer_zip  varchar(45)  YES         None       \n\n****************************** customer_purchases ******************************\n                      Field           Type Null  Key Default Extra\n0                product_id        int(11)   NO  PRI    None      \n1                 vendor_id        int(11)   NO  PRI    None      \n2               market_date           date   NO  PRI    None      \n3               customer_id        int(11)   NO  PRI    None      \n4                  quantity  decimal(16,2)  YES         None      \n5  cost_to_customer_per_qty  decimal(16,2)  YES         None      \n6          transaction_time           time   NO  PRI    None       \n\n****************************** market_date_info ******************************\n                Field          Type Null  Key Default Extra\n0         market_date          date   NO  PRI    None      \n1          market_day   varchar(45)  YES         None      \n2         market_week   varchar(45)  YES         None      \n3         market_year   varchar(45)  YES         None      \n4   market_start_time   varchar(45)  YES         None      \n5     market_end_time   varchar(45)  YES         None      \n6       special_notes          blob  YES         None      \n7       market_season   varchar(45)  YES         None      \n8     market_min_temp  varchar(200)  YES         None      \n9     market_max_temp   varchar(45)  YES         None      \n10   market_rain_flag       int(11)  YES         None      \n11   market_snow_flag       int(11)  YES         None       \n\n****************************** product ******************************\n                 Field         Type Null  Key Default Extra\n0           product_id      int(11)   NO  PRI    None      \n1         product_name  varchar(45)  YES         None      \n2         product_size  varchar(45)  YES         None      \n3  product_category_id      int(11)   NO  PRI    None      \n4     product_qty_type  varchar(45)  YES         None       \n\n****************************** product_category ******************************\n                   Field         Type Null  Key Default           Extra\n0    product_category_id      int(11)   NO  PRI    None  auto_increment\n1  product_category_name  varchar(45)  YES         None                 \n\n****************************** vendor ******************************\n                     Field         Type Null  Key Default           Extra\n0                vendor_id      int(11)   NO  PRI    None  auto_increment\n1              vendor_name  varchar(45)   NO  UNI    None                \n2              vendor_type  varchar(45)   NO         None                \n3  vendor_owner_first_name  varchar(45)   NO         None                \n4   vendor_owner_last_name  varchar(45)   NO         None                 \n\n****************************** vendor_booth_assignments ******************************\n          Field     Type Null  Key Default Extra\n0     vendor_id  int(11)   NO  PRI    None      \n1  booth_number  int(11)   NO  PRI    None      \n2   market_date     date   NO  PRI    None       \n\n****************************** vendor_inventory ******************************\n            Field           Type Null  Key Default Extra\n0     market_date           date   NO  PRI    None      \n1        quantity  decimal(16,2)  YES         None      \n2       vendor_id        int(11)   NO  PRI    None      \n3      product_id        int(11)   NO  PRI    None      \n4  original_price  decimal(16,2)  YES         None       \n\n****************************** zip_data ******************************\n                     Field     Type Null  Key Default Extra\n0               zip_code_5  char(5)   NO  PRI    None      \n1  median_household_income    float  YES         None      \n2      percent_high_income    float  YES         None      \n3         percent_under_18    float  YES         None      \n4          percent_over_65    float  YES         None      \n5       people_per_sq_mile    float  YES         None      \n6                 latitude    float  YES         None      \n7                longitude    float  YES         None       \n\n\n\n\nproduct = pd.read_sql('select * from product', db)\nproduct.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 23 entries, 0 to 22\nData columns (total 5 columns):\n #   Column               Non-Null Count  Dtype \n---  ------               --------------  ----- \n 0   product_id           23 non-null     int64 \n 1   product_name         23 non-null     object\n 2   product_size         22 non-null     object\n 3   product_category_id  23 non-null     int64 \n 4   product_qty_type     21 non-null     object\ndtypes: int64(2), object(3)\nmemory usage: 1.0+ KB\n\n\n\npd.read_sql('select product_id from product limit 5', db)\n\n\n\n\n\n  \n    \n      \n      product_id\n    \n  \n  \n    \n      0\n      1\n    \n    \n      1\n      2\n    \n    \n      2\n      3\n    \n    \n      3\n      9\n    \n    \n      4\n      12"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My latest projects",
    "section": "",
    "text": "My latest projects\n\n\n\n\n\n\n \n\n\n\nExtracting and grouping text info by category\n\n\n\ndata cleaning\n\n\nregex\n\n\n\nThis notebook shows matching text pattern and extract text and processing strings using Regex.\n\n\n\nOct 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGenerate English name\n\n\n\ndeep learning\n\n\nnlp\n\n\n\nTrain a language model to generate english names\n\n\n\nSep 23, 2022\n\n\n\n\n\n\n\n\n\n\n \n\n\n\nHouse Price Prediction\n\n\n\nmachine learning\n\n\n\nThis notebook will\n\n\n\nMay 23, 2022\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nMy latest posts\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe common activation functions\n\n\n\n\n\n\n\ndeep learning\n\n\nmath\n\n\n\n\nBasic math for deep learning\n\n\n\n\n\n\nNov 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHandling missing data\n\n\n\n\n\n\n\npython\n\n\npandas\n\n\nnumpy\n\n\n\n\nExamples of dealing with missing data\n\n\n\n\n\n\nOct 30, 2022\n\n\n\n\n\n\n  \n\n\n\n\nConnect to MySql with Pandas\n\n\n\n\n\n\n\nmysql\n\n\ndatabase\n\n\npandas\n\n\n\n\nThis blog show how to connect to MySql database via Pandas\n\n\n\n\n\n\nSep 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Huggingface datasets\n\n\n\n\n\n\n\nnlp\n\n\nhuggingface\n\n\n\n\nThis blog will dive into huggingface datasets\n\n\n\n\n\n\nAug 19, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "tils/index.html",
    "href": "tils/index.html",
    "title": "TILs",
    "section": "",
    "text": "Today I learned\n\n\n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\n\n\n\n\nNov 17, 2022\n\n\nDefine a spark schema\n\n\n\n\nNov 15, 2022\n\n\nFind the path of a Linux command\n\n\n\n\nNov 15, 2022\n\n\nSet up spark in Jupyter Notebook\n\n\n\n\nNov 10, 2022\n\n\nImage data augmentation\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "tils/nbs/image-data-augmentation.html",
    "href": "tils/nbs/image-data-augmentation.html",
    "title": "Image data augmentation",
    "section": "",
    "text": "This notebook will show the common methods used for preparing image data for vision models in PyTorch.\n\nLoad an original image\n\nimg = Image.open('images/cat.jpeg')\nimg\n\n\n\n\n\n\nConvert the image to Tensor\n\ntoTensor = torchvision.transforms.ToTensor()\ntoTensor(img).shape\n\ntorch.Size([3, 1199, 1200])\n\n\nFrom above, we know that the image size is 1199 * 1200.\n\n\nResize the image to 224 by 224\n\nresize = torchvision.transforms.Resize(224)\nimg_rs = resize(img)\nimg_rs\n\n\n\n\n\n\nFlip an image\nFlip horizontally\n\nflip = torchvision.transforms.RandomHorizontalFlip(p=1.0)\nflip(img_rs)\n\n\n\n\nFlip vertically\n\nflip = torchvision.transforms.RandomVerticalFlip(p=1.0)\nflip(img_rs)\n\n\n\n\n\n\nChange brightness, contrast, saturation and hue of an image\n\ncolorjitter = torchvision.transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.3)\ncolorjitter(img_rs)\n\n\n\n\n\n\nTurn an image grayscale\n\ngrayscale = torchvision.transforms.Grayscale()\ngrayscale(img_rs)\n\n\n\n\n\n\nCrop an image\n\ncrop = torchvision.transforms.CenterCrop(128)\ncrop(img_rs)"
  },
  {
    "objectID": "tils/nbs/define-a-spark-schema.html",
    "href": "tils/nbs/define-a-spark-schema.html",
    "title": "Define a spark schema",
    "section": "",
    "text": "Setup\n\nfrom pyspark.sql.types import *\nfrom pyspark.sql import SparkSession\n\n\nspark = SparkSession.builder.appName('Simple-table').getOrCreate()\n\n\n\nDefine it programmatically\n\nschema = StructType([StructField(\"first name\", StringType(), False), \n                     StructField(\"last name\", StringType(), False), \n                     StructField(\"weight\", IntegerType(), False)])\n\n\nschema\n\nStructType([StructField('first name', StringType(), False), StructField('last name', StringType(), False), StructField('weight', IntegerType(), False)])\n\n\nFalse indicate whether the field can be null (None) or not.\n\ndata = [['Jake', 'Z', 60], ['Tom', 'X', 50]]\n\n\ndf = spark.createDataFrame(data, schema)\ndf.show()\n\n+----------+---------+------+\n|first name|last name|weight|\n+----------+---------+------+\n|      Jake|        Z|    60|\n|       Tom|        X|    50|\n+----------+---------+------+\n\n\n\n\n\nDefine it using DDL\nThis method is much simper.\n\nschema = \"first_name STRING, last_name STRING, weight INT\"\n\n\nschema\n\n'first_name STRING, last_name STRING, weight INT'\n\n\n\ndf = spark.createDataFrame(data, schema)\ndf.show()\n\n+----------+---------+------+\n|first_name|last_name|weight|\n+----------+---------+------+\n|      Jake|        Z|    60|\n|       Tom|        X|    50|\n+----------+---------+------+\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nA disadvantage of this method is we cannot put space or - between words.\n\n\n\nspark.stop()"
  },
  {
    "objectID": "tils/nbs/find-path-of-a-linux-command.html",
    "href": "tils/nbs/find-path-of-a-linux-command.html",
    "title": "Find the path of a Linux command",
    "section": "",
    "text": "Suppose we want to find the path of the command spark-submit in terminal, we can do it in 4 ways.\n\nwhich command\n\n!which spark-submit\n\n/Users/youfeng/mambaforge/bin/spark-submit\n\n\nWith a parameter -a will print all matching paths. In this case, there is only one.\n\n!which -a spark-submit\n\n/Users/youfeng/mambaforge/bin/spark-submit\n\n\n\n\ncommand command\n\n!command -v spark-submit\n\n/Users/youfeng/mambaforge/bin/spark-submit\n\n\n\n!command -V spark-submit\n\nspark-submit is /Users/youfeng/mambaforge/bin/spark-submit\n\n\n\n\n\n\n\n\nWarning\n\n\n\nWe must pass the parameter -v or -V, otherwise, it will run the command.\n\n\n\n\ntype command\nShow the path\n\n!type -p spark-submit\n\nspark-submit is /Users/youfeng/mambaforge/bin/spark-submit\n\n\nShow the definition\n\n!type spark-submit\n\nspark-submit is /Users/youfeng/mambaforge/bin/spark-submit\n\n\nShow the definition, excutable type, and path\n\n!type -a spark-submit\n\nspark-submit is /Users/youfeng/mambaforge/bin/spark-submit\n\n\n\n\nwhereis command\nShow all the locations of the binary, source, manual page\n\n!whereis spark-submit\n\nspark-submit: /Users/youfeng/mambaforge/bin/spark-submit\n\n\nShow the locations of the binary\n\n!whereis -b spark-submit\n\nspark-submit: /Users/youfeng/mambaforge/bin/spark-submit\n\n\nShow the locations of the source"
  },
  {
    "objectID": "tils/nbs/setup-spark-in-jupyter-notebook.html",
    "href": "tils/nbs/setup-spark-in-jupyter-notebook.html",
    "title": "Set up spark in Jupyter Notebook",
    "section": "",
    "text": "Low level API\n\nfrom pyspark import SparkContext, SparkConf\n\n\nconf = SparkConf().setMaster('local')\nsc = SparkContext(conf=conf)\n\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n\n\n22/11/17 11:17:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n\n\n\nsc.stop()\n\n\n\nHigh level API\n\nfrom pyspark.sql import SparkSession\n\n\nspark = SparkSession.builder.appName('chapter2').getOrCreate()\n\n\nspark.stop()"
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "My projects\n\n\n\n\n\n\n\n\n\n\n\nExtracting and grouping text info by category\n\n\n\ndata cleaning\n\n\nregex\n\n\n\nThis notebook shows matching text pattern and extract text and processing strings using Regex.\n\n\n\nOct 23, 2022\n\n\n\n\n\n\n\n\n\n\n\nGenerate English name\n\n\n\ndeep learning\n\n\nnlp\n\n\n\nTrain a language model to generate english names\n\n\n\nSep 23, 2022\n\n\n\n\n\n\n\n\n\n\n\nHouse Price Prediction\n\n\n\nmachine learning\n\n\n\nThis notebook will\n\n\n\nMay 23, 2022\n\n\n\n\n\n\n\n\n\n\n\nWords Clouds\n\n\n\nexploratory data analysis\n\n\n\nThis notebook will show how to create word clouds in python\n\n\n\nMay 23, 2022\n\n\n\n\n\n\n\n\n\n\n\nPower BI\n\n\n\ndata visualization\n\n\n\nThis notebook will show how to use Power BI\n\n\n\nMay 23, 2022\n\n\n\n\n\n\n\n\n\n\n\nEigenFace\n\n\n\nmachine learning\n\n\n\nThis notebook will\n\n\n\nMay 23, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/nbs/house_price_prediction.html",
    "href": "projects/nbs/house_price_prediction.html",
    "title": "House Price Prediction",
    "section": "",
    "text": "from fastai.tabular.all import *"
  },
  {
    "objectID": "projects/nbs/extract_dataframe_from_txt_file.html",
    "href": "projects/nbs/extract_dataframe_from_txt_file.html",
    "title": "Extracting and grouping text info by category",
    "section": "",
    "text": "About the text file:\nThe text file we will work on is a data description file which is part of the Kaggle dataset. The file is used to describe all of the 79 explanatory variables (features) and the meaning of each corresponding category variable’s code.\n\n\nThe goal\nThe goal of this project is to extract the information about the explanatory variables and category variables’ codes, then create a seperate pandas dataframe for each one and save as csv file for downstream tasks.\n\n\nImport packages\n\nimport pandas as pd\nimport numpy as np\nimport re\nfrom fastcore.utils import *\n\n\n\nLoad the data\nThe path of the Kaggle dataset.\n\npath = Path('./house-prices-advanced-regression-techniques/')\n\nHave a look at the data\n\ntrain_df = pd.read_csv(path/'train.csv')\ntrain_df.shape\n\n(1460, 81)\n\n\n\ntrain_df.tail().iloc[:, -5:]\n\n\n\n\n\n  \n    \n      \n      MoSold\n      YrSold\n      SaleType\n      SaleCondition\n      SalePrice\n    \n  \n  \n    \n      1455\n      8\n      2007\n      WD\n      Normal\n      175000\n    \n    \n      1456\n      2\n      2010\n      WD\n      Normal\n      210000\n    \n    \n      1457\n      5\n      2010\n      WD\n      Normal\n      266500\n    \n    \n      1458\n      4\n      2010\n      WD\n      Normal\n      142125\n    \n    \n      1459\n      6\n      2008\n      WD\n      Normal\n      147500\n    \n  \n\n\n\n\n\ntest_df = pd.read_csv(path/'test.csv')\ntest_df.shape\n\n(1459, 80)\n\n\n\ntest_df.tail().iloc[:, -5:]\n\n\n\n\n\n  \n    \n      \n      MiscVal\n      MoSold\n      YrSold\n      SaleType\n      SaleCondition\n    \n  \n  \n    \n      1454\n      0\n      6\n      2006\n      WD\n      Normal\n    \n    \n      1455\n      0\n      4\n      2006\n      WD\n      Abnorml\n    \n    \n      1456\n      0\n      9\n      2006\n      WD\n      Abnorml\n    \n    \n      1457\n      700\n      7\n      2006\n      WD\n      Normal\n    \n    \n      1458\n      0\n      11\n      2006\n      WD\n      Normal\n    \n  \n\n\n\n\n\ntrain_df.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1460 entries, 0 to 1459\nData columns (total 81 columns):\n #   Column         Non-Null Count  Dtype  \n---  ------         --------------  -----  \n 0   Id             1460 non-null   int64  \n 1   MSSubClass     1460 non-null   int64  \n 2   MSZoning       1460 non-null   object \n 3   LotFrontage    1201 non-null   float64\n 4   LotArea        1460 non-null   int64  \n 5   Street         1460 non-null   object \n 6   Alley          91 non-null     object \n 7   LotShape       1460 non-null   object \n 8   LandContour    1460 non-null   object \n 9   Utilities      1460 non-null   object \n 10  LotConfig      1460 non-null   object \n 11  LandSlope      1460 non-null   object \n 12  Neighborhood   1460 non-null   object \n 13  Condition1     1460 non-null   object \n 14  Condition2     1460 non-null   object \n 15  BldgType       1460 non-null   object \n 16  HouseStyle     1460 non-null   object \n 17  OverallQual    1460 non-null   int64  \n 18  OverallCond    1460 non-null   int64  \n 19  YearBuilt      1460 non-null   int64  \n 20  YearRemodAdd   1460 non-null   int64  \n 21  RoofStyle      1460 non-null   object \n 22  RoofMatl       1460 non-null   object \n 23  Exterior1st    1460 non-null   object \n 24  Exterior2nd    1460 non-null   object \n 25  MasVnrType     1452 non-null   object \n 26  MasVnrArea     1452 non-null   float64\n 27  ExterQual      1460 non-null   object \n 28  ExterCond      1460 non-null   object \n 29  Foundation     1460 non-null   object \n 30  BsmtQual       1423 non-null   object \n 31  BsmtCond       1423 non-null   object \n 32  BsmtExposure   1422 non-null   object \n 33  BsmtFinType1   1423 non-null   object \n 34  BsmtFinSF1     1460 non-null   int64  \n 35  BsmtFinType2   1422 non-null   object \n 36  BsmtFinSF2     1460 non-null   int64  \n 37  BsmtUnfSF      1460 non-null   int64  \n 38  TotalBsmtSF    1460 non-null   int64  \n 39  Heating        1460 non-null   object \n 40  HeatingQC      1460 non-null   object \n 41  CentralAir     1460 non-null   object \n 42  Electrical     1459 non-null   object \n 43  1stFlrSF       1460 non-null   int64  \n 44  2ndFlrSF       1460 non-null   int64  \n 45  LowQualFinSF   1460 non-null   int64  \n 46  GrLivArea      1460 non-null   int64  \n 47  BsmtFullBath   1460 non-null   int64  \n 48  BsmtHalfBath   1460 non-null   int64  \n 49  FullBath       1460 non-null   int64  \n 50  HalfBath       1460 non-null   int64  \n 51  BedroomAbvGr   1460 non-null   int64  \n 52  KitchenAbvGr   1460 non-null   int64  \n 53  KitchenQual    1460 non-null   object \n 54  TotRmsAbvGrd   1460 non-null   int64  \n 55  Functional     1460 non-null   object \n 56  Fireplaces     1460 non-null   int64  \n 57  FireplaceQu    770 non-null    object \n 58  GarageType     1379 non-null   object \n 59  GarageYrBlt    1379 non-null   float64\n 60  GarageFinish   1379 non-null   object \n 61  GarageCars     1460 non-null   int64  \n 62  GarageArea     1460 non-null   int64  \n 63  GarageQual     1379 non-null   object \n 64  GarageCond     1379 non-null   object \n 65  PavedDrive     1460 non-null   object \n 66  WoodDeckSF     1460 non-null   int64  \n 67  OpenPorchSF    1460 non-null   int64  \n 68  EnclosedPorch  1460 non-null   int64  \n 69  3SsnPorch      1460 non-null   int64  \n 70  ScreenPorch    1460 non-null   int64  \n 71  PoolArea       1460 non-null   int64  \n 72  PoolQC         7 non-null      object \n 73  Fence          281 non-null    object \n 74  MiscFeature    54 non-null     object \n 75  MiscVal        1460 non-null   int64  \n 76  MoSold         1460 non-null   int64  \n 77  YrSold         1460 non-null   int64  \n 78  SaleType       1460 non-null   object \n 79  SaleCondition  1460 non-null   object \n 80  SalePrice      1460 non-null   int64  \ndtypes: float64(3), int64(35), object(43)\nmemory usage: 924.0+ KB\n\n\nFrom the above, we know that train set (except the dependent valiable SalePrice) and test set both have 79 explanatory variables (excluding Id).\nAll the independent variables are:\n\nind_vars = train_df.columns[1:-1]\nind_vars\n\nIndex(['MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'Street', 'Alley',\n       'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope',\n       'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle',\n       'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'RoofStyle',\n       'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'MasVnrArea',\n       'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond',\n       'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1', 'BsmtFinType2',\n       'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'Heating', 'HeatingQC',\n       'CentralAir', 'Electrical', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF',\n       'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath',\n       'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual', 'TotRmsAbvGrd',\n       'Functional', 'Fireplaces', 'FireplaceQu', 'GarageType', 'GarageYrBlt',\n       'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual', 'GarageCond',\n       'PavedDrive', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch',\n       'ScreenPorch', 'PoolArea', 'PoolQC', 'Fence', 'MiscFeature', 'MiscVal',\n       'MoSold', 'YrSold', 'SaleType', 'SaleCondition'],\n      dtype='object')\n\n\n\nlen(ind_vars)\n\n79\n\n\nAmong the independent variables, some are numerical variables, others are category variables.\n\nnum_cols = train_df.select_dtypes(include=np.number).columns # numeric columns\nnum_cols\n\nIndex(['Id', 'MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual',\n       'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1',\n       'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF',\n       'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath',\n       'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd',\n       'Fireplaces', 'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF',\n       'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea',\n       'MiscVal', 'MoSold', 'YrSold', 'SalePrice'],\n      dtype='object')\n\n\n\ncat_cols = train_df.select_dtypes(include='object').columns # category columns\ncat_cols\n\nIndex(['MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities',\n       'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2',\n       'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st',\n       'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation',\n       'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n       'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual',\n       'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual',\n       'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature',\n       'SaleType', 'SaleCondition'],\n      dtype='object')\n\n\nLet have a look at the data description file: data_description.txt\nThe first 30 lines is as below:\n\n!head -n 30 {path/'data_description.txt'}\n\nMSSubClass: Identifies the type of dwelling involved in the sale.   \n\n        20  1-STORY 1946 & NEWER ALL STYLES\n        30  1-STORY 1945 & OLDER\n        40  1-STORY W/FINISHED ATTIC ALL AGES\n        45  1-1/2 STORY - UNFINISHED ALL AGES\n        50  1-1/2 STORY FINISHED ALL AGES\n        60  2-STORY 1946 & NEWER\n        70  2-STORY 1945 & OLDER\n        75  2-1/2 STORY ALL AGES\n        80  SPLIT OR MULTI-LEVEL\n        85  SPLIT FOYER\n        90  DUPLEX - ALL STYLES AND AGES\n       120  1-STORY PUD (Planned Unit Development) - 1946 & NEWER\n       150  1-1/2 STORY PUD - ALL AGES\n       160  2-STORY PUD - 1946 & NEWER\n       180  PUD - MULTILEVEL - INCL SPLIT LEV/FOYER\n       190  2 FAMILY CONVERSION - ALL STYLES AND AGES\n\nMSZoning: Identifies the general zoning classification of the sale.\n        \n       A    Agriculture\n       C    Commercial\n       FV   Floating Village Residential\n       I    Industrial\n       RH   Residential High Density\n       RL   Residential Low Density\n       RP   Residential Low Density Park \n       RM   Residential Medium Density\n    \n\n\nLet grep the lines with MS\n\n!grep MS {path/'data_description.txt'}\n\nMSSubClass: Identifies the type of dwelling involved in the sale.   \nMSZoning: Identifies the general zoning classification of the sale.\n\n\n\n!rg MS {path/'data_description.txt'}\n\n1:MSSubClass: Identifies the type of dwelling involved in the sale. \n20:MSZoning: Identifies the general zoning classification of the sale.\n\n\nRead the text file and split lines into a list\n\ndesc = (path/'data_description.txt').read_text().splitlines()\ndesc[:30]\n\n['MSSubClass: Identifies the type of dwelling involved in the sale.\\t',\n '',\n '        20\\t1-STORY 1946 & NEWER ALL STYLES',\n '        30\\t1-STORY 1945 & OLDER',\n '        40\\t1-STORY W/FINISHED ATTIC ALL AGES',\n '        45\\t1-1/2 STORY - UNFINISHED ALL AGES',\n '        50\\t1-1/2 STORY FINISHED ALL AGES',\n '        60\\t2-STORY 1946 & NEWER',\n '        70\\t2-STORY 1945 & OLDER',\n '        75\\t2-1/2 STORY ALL AGES',\n '        80\\tSPLIT OR MULTI-LEVEL',\n '        85\\tSPLIT FOYER',\n '        90\\tDUPLEX - ALL STYLES AND AGES',\n '       120\\t1-STORY PUD (Planned Unit Development) - 1946 & NEWER',\n '       150\\t1-1/2 STORY PUD - ALL AGES',\n '       160\\t2-STORY PUD - 1946 & NEWER',\n '       180\\tPUD - MULTILEVEL - INCL SPLIT LEV/FOYER',\n '       190\\t2 FAMILY CONVERSION - ALL STYLES AND AGES',\n '',\n 'MSZoning: Identifies the general zoning classification of the sale.',\n '\\t\\t',\n '       A\\tAgriculture',\n '       C\\tCommercial',\n '       FV\\tFloating Village Residential',\n '       I\\tIndustrial',\n '       RH\\tResidential High Density',\n '       RL\\tResidential Low Density',\n '       RP\\tResidential Low Density Park ',\n '       RM\\tResidential Medium Density',\n '\\t']\n\n\nRemove all the empty lines.\n\ndesc = [i for i in desc if len(i.strip()) != 0]\ndesc[:30]\n\n['MSSubClass: Identifies the type of dwelling involved in the sale.\\t',\n '        20\\t1-STORY 1946 & NEWER ALL STYLES',\n '        30\\t1-STORY 1945 & OLDER',\n '        40\\t1-STORY W/FINISHED ATTIC ALL AGES',\n '        45\\t1-1/2 STORY - UNFINISHED ALL AGES',\n '        50\\t1-1/2 STORY FINISHED ALL AGES',\n '        60\\t2-STORY 1946 & NEWER',\n '        70\\t2-STORY 1945 & OLDER',\n '        75\\t2-1/2 STORY ALL AGES',\n '        80\\tSPLIT OR MULTI-LEVEL',\n '        85\\tSPLIT FOYER',\n '        90\\tDUPLEX - ALL STYLES AND AGES',\n '       120\\t1-STORY PUD (Planned Unit Development) - 1946 & NEWER',\n '       150\\t1-1/2 STORY PUD - ALL AGES',\n '       160\\t2-STORY PUD - 1946 & NEWER',\n '       180\\tPUD - MULTILEVEL - INCL SPLIT LEV/FOYER',\n '       190\\t2 FAMILY CONVERSION - ALL STYLES AND AGES',\n 'MSZoning: Identifies the general zoning classification of the sale.',\n '       A\\tAgriculture',\n '       C\\tCommercial',\n '       FV\\tFloating Village Residential',\n '       I\\tIndustrial',\n '       RH\\tResidential High Density',\n '       RL\\tResidential Low Density',\n '       RP\\tResidential Low Density Park ',\n '       RM\\tResidential Medium Density',\n 'LotFrontage: Linear feet of street connected to property',\n 'LotArea: Lot size in square feet',\n 'Street: Type of road access to property',\n '       Grvl\\tGravel\\t']\n\n\nAfter looking at the above text file, we noticed that the data types of some independent variables are incorrect.\nLet fix it.\n\n\nDefine a function to extract all the descriptions about the 79 explanatory variables and check the data type.\n\ndef var_desc(txt, ind_vars):\n    d = {}\n    cat = {}\n    for i, t in enumerate(txt):\n        rt = re.findall(r'(\\S+): ([^\\t?]+)', t)\n        if rt:\n            if rt[0][0] in ind_vars:\n                d[rt[0][0]] = rt[0][1]\n                if txt[i+1].startswith(' '):\n                    cat[rt[0][0]] = 'Category'\n    df = pd.DataFrame(d.items(), columns=['Variable', 'Variabe_Description'])\n    df['Variable_Type'] = df.Variable.map(cat).fillna('Numerical')\n    return df\n\n\ndf = var_desc(desc, ind_vars)\ndf.head()\n\n\n\n\n\n  \n    \n      \n      Variable\n      Variabe_Description\n      Variable_Type\n    \n  \n  \n    \n      0\n      MSSubClass\n      Identifies the type of dwelling involved in th...\n      Category\n    \n    \n      1\n      MSZoning\n      Identifies the general zoning classification o...\n      Category\n    \n    \n      2\n      LotFrontage\n      Linear feet of street connected to property\n      Numerical\n    \n    \n      3\n      LotArea\n      Lot size in square feet\n      Numerical\n    \n    \n      4\n      Street\n      Type of road access to property\n      Category\n    \n  \n\n\n\n\n\nlen(df)\n\n77\n\n\n\ndf.to_csv(path/f'vars_desc.csv', index=False)\n\n\ncat_vars = df['Variable'][df['Variable_Type'] == 'Category'].values\ncat_vars, len(cat_vars)\n\n(array(['MSSubClass', 'MSZoning', 'Street', 'Alley', 'LotShape',\n        'LandContour', 'Utilities', 'LotConfig', 'LandSlope',\n        'Neighborhood', 'Condition1', 'Condition2', 'BldgType',\n        'HouseStyle', 'OverallQual', 'OverallCond', 'RoofStyle',\n        'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType',\n        'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond',\n        'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating',\n        'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual',\n        'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish',\n        'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', 'Fence',\n        'MiscFeature', 'SaleType', 'SaleCondition'], dtype=object),\n 46)\n\n\n\ncat_cols, len(cat_cols)\n\n(Index(['MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities',\n        'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2',\n        'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st',\n        'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation',\n        'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n        'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual',\n        'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual',\n        'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature',\n        'SaleType', 'SaleCondition'],\n       dtype='object'),\n 43)\n\n\n\n[i for i in cat_cols if i not in cat_vars]\n\n[]\n\n\n\n[i for i in cat_vars if i not in cat_cols]\n\n['MSSubClass', 'OverallQual', 'OverallCond']\n\n\nAfter comparing the results of data types from the original train set and text file, we get the category variables.\n\ncat_vars\n\narray(['MSSubClass', 'MSZoning', 'Street', 'Alley', 'LotShape',\n       'LandContour', 'Utilities', 'LotConfig', 'LandSlope',\n       'Neighborhood', 'Condition1', 'Condition2', 'BldgType',\n       'HouseStyle', 'OverallQual', 'OverallCond', 'RoofStyle',\n       'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType',\n       'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond',\n       'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating',\n       'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual',\n       'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish',\n       'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', 'Fence',\n       'MiscFeature', 'SaleType', 'SaleCondition'], dtype=object)\n\n\n\n\nDefine a function to extract all the information about each category variable’s codes.\n\ndef cat_desc(txt, col_name):\n    d = {}\n    # find the column name match the category variable\n    for i in range(len(txt)):\n        rt1 = re.match(col_name, txt[i])\n        if rt1:\n#             print(rt1.group(0), i) # debugging\n            j = i+1\n    # extract the info about the the corresponding category variable\n    for t in txt[j:]:\n        if t.startswith(' '): # check if the text start with white space\n            # find two parts (the code and code description) using regex\n            # the code might be 'NA' , digits or any word characters\n            rt2 = re.findall(r'\\s+(\\d+\\.\\d+\\w+|NA\\s+|\\w+\\&?\\w*?\\s*?)\\t(.*)', t, flags=re.IGNORECASE) \n            d[rt2[0][0].strip()] = (rt2[0][1]).strip()\n        else:\n            break\n    # return a dataframe of the category variable\n    return pd.DataFrame(d.items(), columns=[col_name, col_name + '_Description'])\n\nTest the function with category variable MSSubClass and SaleCondition\n\ncol_name = 'MSSubClass'\ncat_desc(desc, col_name)\n\n\n\n\n\n  \n    \n      \n      MSSubClass\n      MSSubClass_Description\n    \n  \n  \n    \n      0\n      20\n      1-STORY 1946 & NEWER ALL STYLES\n    \n    \n      1\n      30\n      1-STORY 1945 & OLDER\n    \n    \n      2\n      40\n      1-STORY W/FINISHED ATTIC ALL AGES\n    \n    \n      3\n      45\n      1-1/2 STORY - UNFINISHED ALL AGES\n    \n    \n      4\n      50\n      1-1/2 STORY FINISHED ALL AGES\n    \n    \n      5\n      60\n      2-STORY 1946 & NEWER\n    \n    \n      6\n      70\n      2-STORY 1945 & OLDER\n    \n    \n      7\n      75\n      2-1/2 STORY ALL AGES\n    \n    \n      8\n      80\n      SPLIT OR MULTI-LEVEL\n    \n    \n      9\n      85\n      SPLIT FOYER\n    \n    \n      10\n      90\n      DUPLEX - ALL STYLES AND AGES\n    \n    \n      11\n      120\n      1-STORY PUD (Planned Unit Development) - 1946 ...\n    \n    \n      12\n      150\n      1-1/2 STORY PUD - ALL AGES\n    \n    \n      13\n      160\n      2-STORY PUD - 1946 & NEWER\n    \n    \n      14\n      180\n      PUD - MULTILEVEL - INCL SPLIT LEV/FOYER\n    \n    \n      15\n      190\n      2 FAMILY CONVERSION - ALL STYLES AND AGES\n    \n  \n\n\n\n\n\ncol_name = 'SaleCondition'\ncat_desc(desc, col_name)\n\n\n\n\n\n  \n    \n      \n      SaleCondition\n      SaleCondition_Description\n    \n  \n  \n    \n      0\n      Normal\n      Normal Sale\n    \n    \n      1\n      Abnorml\n      Abnormal Sale -  trade, foreclosure, short sale\n    \n    \n      2\n      AdjLand\n      Adjoining Land Purchase\n    \n    \n      3\n      Alloca\n      Allocation - two linked properties with separa...\n    \n    \n      4\n      Family\n      Sale between family members\n    \n    \n      5\n      Partial\n      Home was not completed when last assessed (ass...\n    \n  \n\n\n\n\n\n\nCreate pandas dataframe and save them as csv files.\n\nfor cat_var in cat_vars:\n    cat_df = cat_desc(desc, cat_var)\n    cat_df.to_csv(path/f'{cat_var}.csv', index=False)\n\n\n!ls {path}\n\nAlley.csv             Foundation.csv        MiscFeature.csv\nBldgType.csv          Functional.csv        Neighborhood.csv\nBsmtCond.csv          GarageCond.csv        OverallCond.csv\nBsmtExposure.csv      GarageFinish.csv      OverallQual.csv\nBsmtFinType1.csv      GarageQual.csv        PavedDrive.csv\nBsmtFinType2.csv      GarageType.csv        PoolQC.csv\nBsmtQual.csv          Heating.csv           RoofMatl.csv\nCentralAir.csv        HeatingQC.csv         RoofStyle.csv\nCondition1.csv        HouseStyle.csv        SaleCondition.csv\nCondition2.csv        KitchenQual.csv       SaleType.csv\nElectrical.csv        LandContour.csv       Street.csv\nExterCond.csv         LandSlope.csv         Utilities.csv\nExterQual.csv         LotConfig.csv         data_description.txt\nExterior1st.csv       LotShape.csv          sample_submission.csv\nExterior2nd.csv       MSSubClass.csv        test.csv\nFence.csv             MSZoning.csv          train.csv\nFireplaceQu.csv       MasVnrType.csv        vars_desc.csv"
  },
  {
    "objectID": "projects/nbs/generate_names.html",
    "href": "projects/nbs/generate_names.html",
    "title": "Generate English name",
    "section": "",
    "text": "Project summary\nThis project train a language model to generate next character based on the previous characters to output an english name.\n\n\nTools used\nPython, PyTorch.\n\n\nRead more\n Project website    GitHub repo"
  },
  {
    "objectID": "projects/nbs/eigenface.html",
    "href": "projects/nbs/eigenface.html",
    "title": "EigenFace",
    "section": "",
    "text": "First\nload data\n\n\nSecond\nLook at the data"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, I am Youfeng.\nWelcome to my personal website. I am sharing my study here."
  }
]